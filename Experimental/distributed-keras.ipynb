{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook describes how to implement keras code with TF1.4 APIs.\n",
    "\n",
    "Unfortunately, **distributed training with tf.keras is not supported** and **exporting model is only supported by latest tensorflow (nightly-built version)**.\n",
    "\n",
    "Content of this notebook is shown below.\n",
    "\n",
    "0. Create TFRecords Files\n",
    "1. Define dataset and global constants\n",
    "2. Define data input function\n",
    "3. Define feature columns\n",
    "4. Instantiate an Estimator\n",
    "5. Train, evaluate and export ML models\n",
    "6. Evaluate with Estimator\n",
    "7. Prediction with Exported Model\n",
    "8. Distributed Training with Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow -y -q\n",
    "!pip install tf-nightly -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create TFRecords Files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_FILENAME = 'cifar-10-python.tar.gz'\n",
    "CIFAR_DOWNLOAD_URL = 'http://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME\n",
    "CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_and_extract(data_dir):\n",
    "  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir, CIFAR_DOWNLOAD_URL)\n",
    "  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME), 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_file_names():\n",
    "  \"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n",
    "  file_names = {}\n",
    "  file_names['train'] = ['data_batch_%d' % i for i in xrange(1, 5)]\n",
    "  file_names['validation'] = ['data_batch_5']\n",
    "  file_names['eval'] = ['test_batch']\n",
    "  return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_pickle_from_file(filename):\n",
    "  with tf.gfile.Open(filename, 'r') as f:\n",
    "    data_dict = cPickle.load(f)\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_tfrecord(input_files, output_file):\n",
    "  \"\"\"Converts a file to TFRecords.\"\"\"\n",
    "  print('Generating %s' % output_file)\n",
    "  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n",
    "    for input_file in input_files:\n",
    "      data_dict = _read_pickle_from_file(input_file)\n",
    "      data = data_dict['data']\n",
    "      labels =  data_dict['labels']\n",
    "      num_entries_in_batch = len(labels)\n",
    "      for i in range(num_entries_in_batch):\n",
    "        example = tf.train.Example(features=tf.train.Features(\n",
    "          feature={\n",
    "            'image': _bytes_feature(data[i].tobytes()),\n",
    "            'label': _int64_feature(labels[i])\n",
    "          }))\n",
    "        record_writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_files(data_dir='cifar-10'):\n",
    "  _download_and_extract(data_dir)\n",
    "  file_names = _get_file_names()\n",
    "  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n",
    "\n",
    "  for mode, files in file_names.items():\n",
    "    input_files = [os.path.join(input_dir, f) for f in files]\n",
    "    output_file = os.path.join(data_dir, mode+'.tfrecords')\n",
    "    try:\n",
    "      os.remove(output_file)\n",
    "    except OSError:\n",
    "      pass\n",
    "    # Convert to tf.train.Example and write to TFRecords.\n",
    "    _convert_to_tfrecord(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tfrecords_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define dataset and global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Global constants describing model behaviors\n",
    "MODEL_NAME = 'distributed-keras'\n",
    "USE_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = ['cifar-10/train.tfrecords']\n",
    "valid_data_files = ['cifar-10/validation.tfrecords']\n",
    "test_data_files = ['cifar-10/eval.tfrecords']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. parsing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(serialized_example):\n",
    "  features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      'image': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  \n",
    "  image = tf.decode_raw(features['image'], tf.uint8)\n",
    "  image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])\n",
    "  image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n",
    "  \n",
    "  label = tf.cast(features['label'], tf.int32)\n",
    "  label = tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. preprocessing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, is_training=False):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_fn(file_names, mode=tf.estimator.ModeKeys.EVAL, batch_size=1):\n",
    "  def _input_fn():\n",
    "    dataset = tf.data.TFRecordDataset(filenames=file_names)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    if is_training:\n",
    "      buffer_size = batch_size * 2 + 1\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    # Transformation\n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.map(\n",
    "      lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "    images, labels = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = tf.keras.models.Sequential()\n",
    "  # Define input tensor in Keras world.\n",
    "  model.add(tf.keras.layers.InputLayer(\n",
    "    input_shape=(IMAGE_WIDTH, IMAGE_HEIGHT, IMAGE_DEPTH), name='images'))\n",
    "\n",
    "  # The first convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "  # The second convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  # The third convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "  # The fourth convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "    filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "  model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
    "  model.add(tf.keras.layers.Activation('softmax'))\n",
    "  \n",
    "  opt = tf.keras.optimizers.RMSprop(lr=0.0001, decay=1e-6)\n",
    "  model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train, Evaluate and Export ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "  keep_checkpoint_max=5,\n",
    "  tf_random_seed=19851211\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Define Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "  receiver_tensor = {'images': tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)}\n",
    "  features = {'images': tf.map_fn(preprocess_image, receiver_tensor['images'])}\n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set learning phase as Training\n",
    "tf.keras.backend.set_learning_phase(True)\n",
    "\n",
    "# Get model defined with tf.keras\n",
    "keras_model = get_model()\n",
    "\n",
    "# Create estimator from keras model\n",
    "estimator = tf.keras.estimator.model_to_estimator(\n",
    "  keras_model=keras_model, model_dir=model_dir, config=run_config)\n",
    "\n",
    "# Currently (2017.12.14) the latest tf only support exporter with keras models.\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "  name='Servo',\n",
    "  serving_input_receiver_fn=serving_input_fn,\n",
    "  assets_extra=None,\n",
    "  as_text=False,\n",
    "  exports_to_keep=5)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn=generate_input_fn(file_names=train_data_files,\n",
    "                             mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                             batch_size=100),\n",
    "  max_steps=1000,\n",
    "  hooks=None\n",
    ")\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn=generate_input_fn(file_names=valid_data_files,\n",
    "                             mode=tf.estimator.ModeKeys.EVAL,\n",
    "                             batch_size=100),\n",
    "  steps=50,\n",
    "  name=None,\n",
    "  hooks=None,\n",
    "  exporters=exporter, # Iterable of Exporters, or single one or None.\n",
    "  start_delay_secs=120,\n",
    "  throttle_secs=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CHECKPOINT:\n",
    "  print(\"Removing previous artifacts...\")\n",
    "  shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate with Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 1000\n",
    "\n",
    "test_input_fn = generate_input_fn(file_names=test_data_files,\n",
    "                                  mode=tf.estimator.ModeKeys.EVAL,\n",
    "                                  batch_size=test_size)\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print(test_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction with Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = model_dir + '/export/Servo/'\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir,\n",
    "  signature_def_key=tf.saved_model.signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will use the first and the last layers' name\n",
    "keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "data_dict = _read_pickle_from_file('cifar-10/cifar-10-batches-py/test_batch')\n",
    "\n",
    "N = 1000\n",
    "images = data_dict['data'][:N].reshape([N, 3, 32, 32]).transpose([0, 2, 3, 1])\n",
    "labels = data_dict['labels'][:N]\n",
    "\n",
    "output = predictor_fn({'images': images})\n",
    "\n",
    "accuracy = numpy.sum(\n",
    "  [ans==ret for ans, ret in zip(labels, numpy.argmax(output['activation_4'], axis=1))]) / float(N)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Distributed Training with Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, tf.keras currently doesn't support distributed training. [The Issue](https://github.com/tensorflow/tensorflow/issues/14504) has already been assigned to Keras author."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT = 'project-id' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'bucket-name' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'bucket-region' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "PROJECT = 'yaboo-sandbox'\n",
    "BUCKET = 'yaboo-sandbox-poc-for-lg'\n",
    "REGION = 'us-central1'\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set permission to BUCKET (NOTE: Create bucket beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print response['serviceAccount']\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Copy TFRecords files to GCS BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo ${BUCKET}\n",
    "gsutil -m rm -rf gs://${BUCKET}/cifar-10\n",
    "gsutil -m cp cifar-10/*.tfrecords gs://${BUCKET}/cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run distributed training with Cloud MLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Success] with Keras, BASIC (single), HEAD (1.5.0-dev20171207)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://$BUCKET/trained_models_dk1\n",
    "JOBNAME=sm_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name='distributed-keras.task' \\\n",
    "   --package-path='trainer/distributed-keras' \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC \\\n",
    "   --runtime-version=HEAD \\\n",
    "   -- \\\n",
    "   --bucket_name=$BUCKET \\\n",
    "   --train_data_pattern='cifar-10/train*.tfrecords' \\\n",
    "   --eval_data_pattern='cifar-10/eval*.tfrecords'  \\\n",
    "   --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Fail] Keras, BASIC_GPU (single), HEAD (1.5.0-dev20171208)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://$BUCKET/trained_models_dk2\n",
    "JOBNAME=sm_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name='distributed-keras.task' \\\n",
    "   --package-path='trainer/distributed-keras' \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=BASIC_GPU \\\n",
    "   --runtime-version=HEAD \\\n",
    "   -- \\\n",
    "   --bucket_name=$BUCKET \\\n",
    "   --train_data_pattern='cifar-10/train*.tfrecords' \\\n",
    "   --eval_data_pattern='cifar-10/eval*.tfrecords'  \\\n",
    "   --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [Failed] Keras, STANDARD_1 (distributed), HEAD (1.5.0-dev20171208)\n",
    "\n",
    "InvalidArgumentError (see above for traceback): Cannot assign a device for operation 'loss/activation_1_loss/sub': Operation was explicitly assigned to /job:ps/task:2 but available devices are [ /job:localhost/replica:0/task:0/device:CPU:0 ]. Make sure the device specification refers to a valid device. `[[Node: loss/activation_1_loss/sub = Sub[T=DT_FLOAT, _device=\"/job:ps/task:2\"](loss/activation_1_loss/sub/x, loss/activation_1_loss/Const)]]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "OUTDIR=gs://$BUCKET/trained_models_dk3\n",
    "JOBNAME=sm_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name='distributed-keras.task' \\\n",
    "   --package-path='trainer/distributed-keras' \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=HEAD \\\n",
    "   -- \\\n",
    "   --bucket_name=$BUCKET \\\n",
    "   --train_data_pattern='cifar-10/train*.tfrecords' \\\n",
    "   --eval_data_pattern='cifar-10/eval*.tfrecords'  \\\n",
    "   --output_dir=$OUTDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Root Cause?\n",
    "\n",
    "As shown above, tf.keras doesn't work in distributed fashion now. The same issue can be found on [GitHub Issue](https://github.com/tensorflow/tensorflow/issues/14504). Minimal example is shown here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "simulate_cluster = True # InvalidArgumentError will happen if this is True\n",
    "if simulate_cluster:\n",
    "    os.environ[\"TF_CONFIG\"] = '{\"environment\": \"cloud\", \"cluster\": {\"worker\": [\"localhost:27184\", \"localhost:27185\"], \\\n",
    "               \"ps\": [\"localhost:27183\"], \"master\": [\"localhost:27182\"]}, \"job\": {\"args\": [\"\"], \\\n",
    "               \"job_name\": \"trainer.task\"}, \"task\": {\"index\": 0, \"type\": \"master\"}}'\n",
    "else:\n",
    "    os.environ[\"TF_CONFIG\"] = ''\n",
    "\n",
    "inputs = tf.keras.layers.Input(shape=(10,))\n",
    "outputs = tf.keras.layers.Dense(10)(inputs)\n",
    "model = tf.keras.models.Model(inputs, outputs)\n",
    "model.compile(optimizer='Adam', loss='binary_crossentropy')\n",
    "est_keras = tf.keras.estimator.model_to_estimator(keras_model=model) # InvalidArgumentError thrown here if simulate_cluster is True\n",
    "\n",
    "input_name = model.input_names[0]\n",
    "data = np.random.rand(1000,10).astype(np.float32)\n",
    "train_input_fn = tf.estimator.inputs.numpy_input_fn({input_name:data}, data, batch_size=10, num_epochs=None, shuffle=False)\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=100)\n",
    "eval_spec = tf.estimator.EvalSpec(input_fn=train_input_fn, steps=10)\n",
    "tf.estimator.train_and_evaluate(est_keras, train_spec, eval_spec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
