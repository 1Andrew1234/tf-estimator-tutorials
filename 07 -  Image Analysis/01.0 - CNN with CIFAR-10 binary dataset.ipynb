{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "from six.moves import urllib\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare CIFAR-10 Dataset (Binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maybe_download_and_extract(\n",
    "  dest_directory='data',\n",
    "  data_url='http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'):\n",
    "  \n",
    "  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "  dest_directory = dest_directory\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = data_url.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-binary.tar.gz 100.0%()\n",
      "('Successfully downloaded', 'cifar-10-binary.tar.gz', 170052171, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data'\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "maybe_download_and_extract(DATA_DIR, DATA_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(index=0, filepath='data/cifar-10-batches-bin/data_batch_5.bin'):\n",
    "  bytestream = open(filepath, mode='rb')\n",
    "\n",
    "  label_bytes_length = 1\n",
    "  image_bytes_length = (32 ** 2) * 3\n",
    "  record_bytes_length = label_bytes_length + image_bytes_length\n",
    "\n",
    "  bytestream.seek(record_bytes_length * index, 0)\n",
    "  label_bytes = bytestream.read(label_bytes_length)\n",
    "  image_bytes = bytestream.read(image_bytes_length)\n",
    "\n",
    "  label = np.frombuffer(label_bytes, dtype=np.uint8)  \n",
    "  image = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  \n",
    "  image = np.reshape(image, [3, 32, 32])\n",
    "  image = np.transpose(image, [1, 2, 0])\n",
    "  image = image.astype(np.float32)\n",
    "  \n",
    "  result = {\n",
    "    'image': image,\n",
    "    'label': label,\n",
    "  }\n",
    "  bytestream.close()\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe51828f5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAIABJREFUeJztnWmMXNd15/+n9qqu7mr23k02d2qzYtMCR5Zjx+M4k0DxJJENDAz7g6EAQhgMYmAMZD4IHmDsAeaDMxjb8IeBB3IsRB44tuXYHgsDYWJZcKIxEi2ULGqjJVJUc+2Fzd6Xqq7lzIcuIlT7/l+X2GQ15ff/AQSr76lb79Z977xX7/7fOcfcHUKI+JHY7gEIIbYHOb8QMUXOL0RMkfMLEVPk/ELEFDm/EDFFzi9ETJHzCxFT5PxCxJTUVjqb2b0Avg4gCeCv3f3LUe/PpJKey4Q3Wa/XI7ZDPi+dpn3WIj6vVuO2BNkWAKST4XNlOpXk46g1rmkcySQfSMRHIp0IjzGb4XNVrdWobS3Clo6Y/1wqPI6o71yPeNi04RFfOoIEOXgiphdRD702Imy5NHenRNSBRcYY9Z3rjbBtcbWK8lo9YmP/wjU7v5klAfwPAL8P4DyA58zsMXd/jfXJZVK459ahoG1+fp5uiznd8M7wZwHAhRn+eZcuz1FbIWIHDpfywfbBvlLEOBapbWp6gdp6u3K83wp3oMGO8BgPjQ7TPhemZ6jt3NRlatsVMf+H+ovB9kuXZ2mf5TV+sK+UK9Tm7OoAIJcOn5g7+XkL9Yiza7nK+x0cGaS2YpZv0DLh43t5bYX2WVgpB9t/9PRZ2mcjW/nZfzeAU+5+2t3XAHwPwH1b+DwhRBvZivPvBHDuqr/PN9uEEO8CtnTP3wpmdhTAUYD/BBNCtJ+tXPkvABi96u9dzba34e4PufsRdz8StTAmhGgvW3H+5wAcMrN9ZpYB8GkAj12fYQkhbjTX/LPf3Wtm9jkAf491qe9hd391k15oIKyVdJe6aC8jkkcyQpPpymSpbcH4OS9KUJpnqtf8Eu0Tpbn0dxeorSfHx7i8skZt9Wp4OXppeZn2SSf5PBYz1IREja/Av3kuvL1alUuHIxGr5TbDFYnLs1zZyXeFVYe+vn7ap77G1ZSJKT6OyUU+jsvLEb9618Ir97sGemiX20bCSksuPc63s4Et3fO7++MAHt/KZwghtgc94SdETJHzCxFT5PxCxBQ5vxAxRc4vREy54U/4XU2tXsc0kWyG+3ppvxQ5RyUjzl09xU5qK5fC0goAeJprW1UiU16ICFbJZsKBNgBQJBGOAJCKiMIrdfLvnUyF+62U+XeOCqqKGuNQH5eiLs2HZcC1Gp+rWoVLmIUcl26rRT7HhXy4XzLNPy9hXOrrj/jO1QjpOao+RqMelgHnF7mE7ET+rkVEYW5EV34hYoqcX4iYIucXIqbI+YWIKXJ+IWJKW1f7k8kkSt3hlFdROfzQCNtWlniwSs35eS1tfOU1n+NTMreyGmxPJvnK8XKVh/asrfHAmFKej6MaES5UJznyuos8iChT5inDInMwGLfVSfBUtcbzYHmDr1R3lXiqtFIPX4FfI0EzE9NcdahW+H7piMhbWMzyeRzoH6C2qZnpYPv5KR6k00lW+9fqrec61JVfiJgi5xcipsj5hYgpcn4hYoqcX4iYIucXIqa0XerbQWSZVIRE0UmCXKYmL9E+C2tcNhod3EFt6YgsfkkSd1Lo5DLaxfmwPAgAqagKThGBIOVVHqSzSIJBGlUe6NTT00dtySSX88okIAUAkqnwodXdxccRJfWdG+f7uhGRk7FeDc+VJ3gAV98OfnwUclzqW1jkY1y8yKvvdBbCc9LdwYPduvPdwfZUYor22Yiu/ELEFDm/EDFFzi9ETJHzCxFT5PxCxBQ5vxAxZUtSn5mNAVgEUAdQc/cjUe9fW1vDW2NngraBLl6uq2coHBFVKoVLMQHA8vwitVXKXH7zOo86Gx0IS2JTc3xbVuKRXqUit1XmI/ICRpyyB/eNBNsvX+Jlpk6ePEttnSQKEwAGh3nuvLm5hWB7R4JHzBWz/PMuzfD52NHLo/rSJNLu3Difj5kZHi26b5jLgLftDs89AFQjIjinZsNzNb3E5cEyySdZjYqO3cD10Pl/193DMYlCiJsW/ewXIqZs1fkdwE/N7HkzO3o9BiSEaA9b/dn/YXe/YGYDAJ4ws1+5+1NXv6F5UjgKAOlkVMFqIUQ72dKV390vNP+fAvBjAHcH3vOQux9x9yMpOb8QNw3X7Pxm1mFmnVdeA/gDAK9cr4EJIW4sW/nZPwjgx2Z25XP+1t3/b1SHegNYWglLEWlwuaw3E4726u8NRzYBgDmPvhqfuExtiYiorYuTYblpOiIZ5MHhfmob6eLRgKdnuRRlESXFRnrCc3LHAJeo5pa5DDWzxEtoZUkpLACYmQ//yluocCkqXeERlcUi39buiCjNUj48V2tLvBTW6jKXgnMJPv7FMpeJK2U+x10dYTkyk+bX5s5iR7hPqvXr+TU7v7ufBvC+a+0vhNheJPUJEVPk/ELEFDm/EDFFzi9ETJHzCxFT2prAM5VIoI/UjOvu5XLNioejm85Oc7nmwDCvjdYZUVNtltTjA4AZIkWlEvwcWl7lY1wrc1mxg0g5AHDxEpdF3zp7Idg+UOTyYCrHJcdGhHTU1cf3WW89LBGePsUlTCf15wCgJ0IWnbvEk1amu8PJMUdHhmiflVUuy9WikoxO8QSeUYk/ezpJUtMKT9RaJPsl8Q6eo9OVX4iYIucXIqbI+YWIKXJ+IWKKnF+ImNLW1f500jDQFQ7QWKnxld65lXDAhIEvbfbP8zxs/d18JT0dsVqaT4ZXejtSEave3TzPYDqiXld6LSIAJsFLedXIR05HrGBPnuOr1KXhXdR22/AwtXWUwrn/pifHaZ+U8e/cyVbEASTWeK67i1NhdSHTxUthdfby8mWXI5SF2Yi8kfUaD0xayIbdsFDgCocniXpgrS/368ovREyR8wsRU+T8QsQUOb8QMUXOL0RMkfMLEVPaKvUBgFv4fDM7y/PqZYkkls0k+XZq4RJIAODO+82vcIlwbCocpJPJ8qCNgR4uUU1enqe2SoPLXilSqml9e+EcfntGuSy3Uon4vD23UluiwGXMHlJ+7QP3fID2uXj2LWqrrvL9kmhwmTiVCwdxlWs8QKe2yIO7quDHTtm5O41NROzrSljKHujlcuTyxESwfbXCv9dGdOUXIqbI+YWIKXJ+IWKKnF+ImCLnFyKmyPmFiCmbSn1m9jCAPwIw5e53Ntt6AHwfwF4AYwA+5e68ZlWTugNL1bCsNNAbjgIDgKGO8DkqFVE6qavAz2uW47Yl57LXTCPcLx1R7qp/kUecJQs8unBoB48U7O7nZaH2joRzF95yYC/tExFAiN6d+6nt2Zd/RW1jkxeD7V0dedpnRzf/zkmSiw8A+nbwYyeVCUfTvfz6adrn9TfGqK27i++zgWGeF7AREbXaIDLm6Ytc/q7Ww8dptRaxMzfQypX/bwDcu6HtQQBPuvshAE82/xZCvIvY1Pnd/SkAG4Oi7wPwSPP1IwA+cZ3HJYS4wVzrPf+gu1/JyjCB9Yq9Qoh3EVte8HN3B/jzpmZ21MyOmdmxap3f9wgh2su1Ov+kmQ0DQPN/mtvI3R9y9yPufiSdlLggxM3CtXrjYwDub76+H8BPrs9whBDtohWp77sAPgqgz8zOA/gigC8DeNTMHgBwBsCnWtmYwZFshMs49Zd40sR9A+FEhucuhiObAODcDI9uGkjwxIgdeW4rJsOyXTbNS2HVIpKMzq9yya6R55JNf1c4cg8AzMJ3YMePP0/7LJf5OO5wfn04ExGFd/b8+WD70vwc7ZM2flt4aC9PJJonkXsAsLoULnmVyfGEmp0dfH92ZrkUXFng0tziApd895OIy539fD+fPBMuy2bvoF7Xps7v7p8hpt9reStCiJsO3YQLEVPk/ELEFDm/EDFFzi9ETJHzCxFT2prAM5NKYHdvOCrq4ng4CgwA0mvhPiskyg4A3pwKSzwAMDXN68Xdtps/qfyhQ6PB9nxXD+1z4hz/XmcuhOUwAJieCycLBYApUtsNALqIRFgo8D77Dt1Gbd1D/LuNzI1Q2/Tl6WC7r3GJLRkh9dXqXLp96dUT1Hb6XPj5s74eHgl45y3h/QwAhSSXRVeIrAgAqxGRpPv6w8d3JsMlx0Q1nNzz1fM8ce2vfUbL7xRC/EYh5xcipsj5hYgpcn4hYoqcX4iYIucXIqa0Veqr1+uYXwpLEdkslzXq1bAENNjJk0FWeC5IjE3x6KtnTp6jtl2kdtpwnZ9DM8YlqtEuLnuVurmto8i/dyoZjuoq9fbTPrv3305tlYjabz1FHk23eyC8Ay4bl8qmJsPyIABMTFyitmInT6p554GdwfbViOjCbIPX6lsp8/k4P8377eznUatdneHovbl5Lttdmg7X/qtd5wSeQojfQOT8QsQUOb8QMUXOL0RMkfMLEVPautrvMNTJ+ebQnvCqLADs7wmvfHcmeSDI6EiS2nIRQS4XZhapLZEhn5ni2+os8TJTvd08uKQUUZ6qQVb0AaDQ0RVsf8/hI7RPpsilkcuzG+u1/AvZNF/tv/XWW4Pttf28/NfMHF/dvjjFlYBqNZwXEgBGh8Ir6fXl8Go5AOSyaWpbqfG5nz7Dy5edPc5tCRwItg8N8yCz6dWwslBrtJ4eX1d+IWKKnF+ImCLnFyKmyPmFiClyfiFiipxfiJjSSrmuhwH8EYApd7+z2fYlAH8G4Eq0xRfc/fFNN5ZKorc7LEV1FSPKZBXC0kuHcYkHRAoBgKHe8BgAYPeBsOwCACND4Zx16SwPwkGCn1870jxAp7PESzV1dHNbV09YtusZ4Pn2VnmsCoozXOp76ySXrxbnwoEzpT4+9l13hAOnAOCOBpfYKmVeCqtWCedCXJqZpH1mSf5BAMhGlF87/K94gNHURZ6vsUpyMs7WuGyX2RGeK7vA52IjrVz5/wbAvYH2r7n74ea/TR1fCHFzsanzu/tTAPjpXwjxrmQr9/yfM7OXzOxhM4uInhdC3Ixcq/N/A8ABAIcBjAP4CnujmR01s2Nmdqy81nqiASHEjeWanN/dJ9297u4NAN8EcHfEex9y9yPufiTHno0XQrSda3J+Mxu+6s9PAnjl+gxHCNEuWpH6vgvgowD6zOw8gC8C+KiZHQbgAMYA/HkrG6tVq5gaD5fKWljmepMf2htsvz1Clusf4JFZg0Wez657iJdq6ukJyyuZLP9Fk0ry82sxW6S2jk6+jJIv8WjA5Uq4ZNQbb52lfWD8MKg2+Hdbi8hduFAJ3+KtLvMcfqlunscxGxFpV8xH5TQM7+veQS59FsZ5HseL5/g8Dg/zfbZ7125qY2XKChFl4Ebf8+Fg++vn/5r22cimzu/unwk0f6vlLQghbkr0hJ8QMUXOL0RMkfMLEVPk/ELEFDm/EDGlrQk88/kc7rwzXBpq4Bb6nBCGdu4Jth+85RDtU4ooj5RI8+irZIbLRul0WPZKpfmTi/W1sPQGAGdOnaG2hbJTW3/EGN+amAq2nxjjUWWV1Qq17RviSST7B4apbXh0X7A9keeJSS9f5iEks5cmeL9LYfkYAA7e9p5guznfZx0FHmG6ssDHWF7lUXgdXTxisWHha7Bn+LV5R19YwkwkWndpXfmFiClyfiFiipxfiJgi5xcipsj5hYgpcn4hYkpbpb5CoQN33f2BoO2uj/wx7ZcthKPfamtcouos8QirX712ktpOv86TUmZT4Yi07A6+rYFRHnn4/e8+Sm1dJZ5k9N/82z+kNi+G+2U6uMS2usIj7ebmeE27oYh6gl0DYYnwwG130j6vHn+B2v7x2Seobe4yT8b5nlsPBttZhCYAnJ3mkXvZKq/l6HUemVpb5Yk//99zx4PtjRSXHD/4oXBUXzkimelGdOUXIqbI+YWIKXJ+IWKKnF+ImCLnFyKmtHW1f2l5Gb/4p2eCtprxlc3b94QDSF57ia8Of+DeT1Lbuef/gdp++fOfUdvoUFh12H33x2ifnff8HrX96Z8+QG3ZPN81+R6uBKAjrDzMLvEV/anzF6lttcH7ja/yle9kMZxncGlpmfYZe/MNanvtl09T264hnpOxmA1f3xYiFIITLz5HbSOliNJsSW5bS/IchIf2hZWHVIHnahzpD28rnWr9eq4rvxAxRc4vREyR8wsRU+T8QsQUOb8QMUXOL0RMaaVc1yiAbwMYxHp5rofc/etm1gPg+wD2Yr1k16fcfTbqs9IJw0hHuCTT4gQPpji9cCFsqK3SPuWVS9TWl+R52H7n8H5qS3o4kGj3MM8X2DfAy38NDO3i2wLP/Tc+t0BtuXxYHtrVz3PxHV/l85gq8pJR+w/yoKVkLiyLvnj8RdpnaZlLhxMzc9Q2vPdWaltNhCXkp4/9M+1z8k0e+JXfx/f17bfeQm3JHA+CKmTCPlEo8FyNaISPgbS1Xgm7lSt/DcBfuvsdAO4B8BdmdgeABwE86e6HADzZ/FsI8S5hU+d393F3f6H5ehHACQA7AdwH4JHm2x4B8IkbNUghxPXnHd3zm9leAO8H8AyAQXe/kjN5Auu3BUKIdwktO7+ZFQH8EMDn3f1tNxzu7lhfDwj1O2pmx8zs2HKZPyoqhGgvLTm/maWx7vjfcfcfNZsnzWy4aR8GEKwW4e4PufsRdz/SkePPNwsh2sumzm9mBuBbAE64+1evMj0G4P7m6/sB/OT6D08IcaNoJarvQwA+C+BlM7ui03wBwJcBPGpmDwA4A+BTm32QAcggXNKotrpE+y1Vw/JF3zCP5po4+xa1XZ4i0iGA5YgUaOVGuITWgSLP4ZfPh2UcAFip8ByEBp7zbXHmMrXVEd5ePs+jJnfvCZdDA4CdEeW67vmdf01tf//TJ4Pt0+ffpH3KM7yk2OAgl9jed89vU5vlwtJnKqLk2e49u6ktmeZ5+ian+H7JFni/Snkt2D47xSMP946E58OclwzbyKbO7+6/AOiRyONVhRA3NXrCT4iYIucXIqbI+YWIKXJ+IWKKnF+ImNLWBJ6NRgOrq2EtLZHL0X6WDJ+jaks80qtR59FNtTKX2M5N8vJU86mwbPQne3hUWTIioeJqg8uAqQTfNblEWHIEgKWF8JycneCRjOU1LkNZij+Ylct2UNvIC+EkmJOv/APt45dPU9tQD4+K27UnIhKTSH35HJf6TrzFE5pWS3y/zGb5sVMLPwPXtCWD7fUafyK2vzcsLzdaV/p05Rcirsj5hYgpcn4hYoqcX4iYIucXIqbI+YWIKW2V+hIJQ57E9Cc9HNkEAKlkWAZMR/TJVXiSy2J/N7WtOD8fDg7/VrC9dzdP3FiPkBzLEbpMNcWlzzypgwcAZ8+MB9ufffZ52ufUGyf4trJcjuSCIzA8GI64PJPlvaoNnsDTy3x/5gvhZKEAUGWSaZXLaMUMlzeLeV6Pr9TNx3FxmsuAl2bCeW8rFX58nyBfqxwRKboRXfmFiClyfiFiipxfiJgi5xcipsj5hYgp7V3tN6CYC59vPMFXvteq4VXP8hrv093Jv1rNeX684f4uatvz2x8Mtmc7eA6/lUWe121umudo27E3rCwAQKHE8+p1d4VLb1XLPDnhuTGe73Bh9r3UliD5GAFgNhsOjukt8VyCyX5eGizfO0RtwwO839h4ODdkusFLlL331hFqGx3gSksuzxWatQovATY/F1YCcgWuLCzMh/tEqUsb0ZVfiJgi5xcipsj5hYgpcn4hYoqcX4iYIucXIqZsKvWZ2SiAb2O9BLcDeMjdv25mXwLwZwAuNd/6BXd/PHJjSUNPMRw0Mbtcpv3OnJ/ebJi/xvLundS2sMJlntGdXFIaGg6XcfIkD35ZHP8VtU0c/ydq29HPx98RUR6spycctDQaUdrs0N5d1NYZITcBPPff7GS4JNrpsbO0T4eHc9kBQB/5XgCQS/NgoUZ1OdheX+ESbH+Bf95gN9/XjTqfj4MjfJ91dpB8ghF5HAtEVvzpqzyv5UZa0flrAP7S3V8ws04Az5vZE03b19z9v7e8NSHETUMrtfrGAYw3Xy+a2QkA/LIkhHhX8I7u+c1sL4D3A3im2fQ5M3vJzB42M/67Rghx09Gy85tZEcAPAXze3RcAfAPAAQCHsf7L4Cuk31EzO2ZmxxZWeQIFIUR7acn5zSyNdcf/jrv/CADcfdLd6+7eAPBNAHeH+rr7Q+5+xN2PdOV5hhQhRHvZ1PnNzAB8C8AJd//qVe3DV73tkwBeuf7DE0LcKFpZ7f8QgM8CeNnMXmy2fQHAZ8zsMNblvzEAf77ZB6VTKQwN9AVtHRV+S9DZG5apTp0N56sDgKdfP09tdePnvEaaR519IBGWvRqrPPfczGtczls88Y/UdrI0Sm37776X2s69FY4eu3SRS2xdBS5fJavhqDgAOB0hVf7g238bbH/92LO0T3otLMsBwG/dyUtyHZw6Q215Ikfu6uMReIMZLvXlkhFl4CKkvoGIaMauUni5bGGJS9KXpsPytzdaj+prZbX/FwBCMbCRmr4Q4uZGT/gJEVPk/ELEFDm/EDFFzi9ETJHzCxFT2prAc63WwNh0WDo6tIs/HdxdDyeKzPgA7ZNPh0sgAcD5S9z2yhtj1Nb5+GPB9lv2PBNsB4Dy6RepbW6BS4SnfvYDasuPHKC2sVPhKMKXX/hn2qe3myctLSxxSeyNx39JbQunXg227+3lkld1iZenKmV40tXK2HPUtiMZ7pfdwa97KeukNk/yKMdEgo/f6zyB6vRUOMJwcppH6NVqYTmyHlECbiO68gsRU+T8QsQUOb8QMUXOL0RMkfMLEVPk/ELElLZKfbVaDTMkGik9ymugJUgdv109XKLyiAirekTdutkyl0qe/tn/DraXD3DJ8fbd4ShGALAsl7125HikXdp4BGSF1DVM1LkMtSPDv3O+tkBt2RqXot53SzgSsxaRz6VeIYksAfREJM5cneXRndVqOMptaYF/r8XViBqEyzxqLh+RSHTPLn6MGFExV8s8qq97R1gaTyZav57ryi9ETJHzCxFT5PxCxBQ5vxAxRc4vREyR8wsRU9oq9eWyGdx+MFzvrlrj8kqSJdzkgV4Y6uXSYamzg9rKJFoKAFaJLZfkYy+XK9SWSPJU5r09PMox3eAS0Pz8fLB9ZYFHMu68pZfaOiKyrSeSfB53ZIg0F1F/Lp3ktkaNy7MVIucBQK0ejsKbXeJScITai8k5Hom5I8vHn89yGXPXyGCwvdDFpey5hfB+TiQl9QkhNkHOL0RMkfMLEVPk/ELEFDm/EDFl09V+M8sBeApAtvn+v3P3L5rZPgDfA9AL4HkAn3V3Hj0CoFyp4I03w6WVas6XWDsL4ZXSviJfbd6369pWsCtrPPKk3giv9pcr/GufnwwHMgHA3DJXAqokbyEADE2NUVsxFV75PrSTqweHBniAUWWej39yeobaCsXwZ5Z6eKBTpiMid16V77RkhOyTy4SPkQwp4wUAlufzMRIWqwAAhWSS2rIZbkskwsfB0iovX1avh49Td65W/dp2W3hPBcDH3P19WC/Hfa+Z3QPgrwB8zd0PApgF8EDLWxVCbDubOr+vcyXlbrr5zwF8DMDfNdsfAfCJGzJCIcQNoaV7fjNLNiv0TgF4AsCbAObc/cpvp/MAdt6YIQohbgQtOb+71939MIBdAO4GcFurGzCzo2Z2zMyOLVVaLx8shLixvKPVfnefA/BzAB8E0G1mVxYMdwG4QPo85O5H3P1IMcsXPYQQ7WVT5zezfjPrbr7OA/h9ACewfhL4d8233Q/gJzdqkEKI608rgT3DAB4xsyTWTxaPuvv/MbPXAHzPzP4rgF8C+NZmH+QwlOvh800jzQMf6iQAprOTS0OZBJd/PB1xzqtzqWStHr5tKeS4DJXJ8vJO6Rofh1fL1LY8cZrabhsIS1tDhw/RPqU0DxSaWeC2yiofY9LCczJR5Xn/5lf43PeXeJDLQIlLc8lGWIbt7uBlyFacHzsryxEBRhFBS5kEPw5KhXAQ1I413sf7wpJpOtV6rN6m73T3lwC8P9B+Guv3/0KIdyF6wk+ImCLnFyKmyPmFiClyfiFiipxfiJhi7yQKaMsbM7sE4EpYXx8AHjLWPjSOt6NxvJ132zj2uHu4VtoG2ur8b9uw2TF3P7ItG9c4NA6NQz/7hYgrcn4hYsp2Ov9D27jtq9E43o7G8XZ+Y8exbff8QojtRT/7hYgp2+L8Znavmb1uZqfM7MHtGENzHGNm9rKZvWhmx9q43YfNbMrMXrmqrcfMnjCzk83/ecbNGzuOL5nZheacvGhmH2/DOEbN7Odm9pqZvWpm/6HZ3tY5iRhHW+fEzHJm9qyZHW+O47802/eZ2TNNv/m+mZGaaC3i7m39ByCJ9TRg+wFkABwHcEe7x9EcyxiAvm3Y7kcA3AXglava/huAB5uvHwTwV9s0ji8B+I9tno9hAHc1X3cCeAPAHe2ek4hxtHVOsF6Fsth8nQbwDIB7ADwK4NPN9v8J4N9vZTvbceW/G8Apdz/t66m+vwfgvm0Yx7bh7k8B2Jj3+j6sJ0IF2pQQlYyj7bj7uLu/0Hy9iPVkMTvR5jmJGEdb8XVueNLc7XD+nQDOXfX3dib/dAA/NbPnzezoNo3hCoPuPt58PQEgXLq1PXzOzF5q3hbc8NuPqzGzvVjPH/EMtnFONowDaPOctCNpbtwX/D7s7ncB+EMAf2FmH9nuAQHrZ36sn5i2g28AOID1Gg3jAL7Srg2bWRHADwF83t0Xrra1c04C42j7nPgWkua2ynY4/wUAo1f9TZN/3mjc/ULz/ykAP8b2ZiaaNLNhAGj+P7Udg3D3yeaB1wDwTbRpTswsjXWH+467/6jZ3PY5CY1ju+akue13nDS3VbbD+Z8DcKi5cpkB8GkAj7V7EGbWYWadV14D+AMAr0T3uqE8hvVEqMA2JkS94mxNPok2zImZGdZzQJ5w969eZWrrnLBxtHtO2pY0t10rmBtWMz+O9ZXUNwH8p20aw36sKw3HAbzaznEA+C7Wfz5WsX7v9gDWax4+CeAkgJ8B6NmmcfwvAC8DeAnrzjfchnF8GOs/6V8C8GKvWSLBAAAAWElEQVTz38fbPScR42jrnAB4L9aT4r6E9RPNf77qmH0WwCkAPwCQ3cp29ISfEDEl7gt+QsQWOb8QMUXOL0RMkfMLEVPk/ELEFDm/EDFFzi9ETJHzCxFT/j+rkR7G7XeD3wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe518915d90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "result = extract_data(np.random.randint(1000))\n",
    "plt.imshow(result['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata** and **global constants**\n",
    "2. Define **data input function** to read the data from the source + **apply pre-processing**\n",
    "3. Create TF **feature columns** based on metadata + **extended feature columns**\n",
    "4. Instantiate a **model function** with the required **feature columns, EstimatorSpecs, & parameters**\n",
    "5. Define a **serving function**\n",
    "6. Run **Experiment** by supplying training and validation data, as well as required parameters\n",
    "7. **Evaluate** the model using test data\n",
    "8. Perform **predictions**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS():\n",
    "  pass\n",
    "\n",
    "FLAGS.batch_size = 200\n",
    "FLAGS.max_steps = 1000\n",
    "FLAGS.eval_steps = 100\n",
    "FLAGS.save_checkpoints_steps = 100\n",
    "FLAGS.tf_random_seed = 19851211\n",
    "FLAGS.model_name = 'cnn-model-01'\n",
    "\n",
    "# We use a weight decay of 0.0002, which performs better than the 0.0001 that\n",
    "# was originally suggested.\n",
    "FLAGS.weight_decay = 2e-4\n",
    "FLAGS.momentum = 0.9\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "FLAGS.tower_name = 'tower'\n",
    "FLAGS.use_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Input Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(raw_record):\n",
    "  # Every record consists of a label followed by the image, with a fixed number\n",
    "  # of bytes for each.\n",
    "  label_bytes = 1\n",
    "  image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "  \n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_vector = tf.decode_raw(raw_record, tf.uint8)\n",
    "  \n",
    "  # The first byte represents the label, which we convert from uint8 to int32\n",
    "  # and then to one-hot.\n",
    "  label = tf.cast(record_vector[0], tf.int32)\n",
    "  label = tf.one_hot(label, NUM_CLASSES)\n",
    "  \n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "    record_vector[label_bytes:record_bytes], [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  \n",
    "  # Convert from [depth, height, width] to [height, width, depth], and cast as\n",
    "  # float32.\n",
    "  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "  \n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, is_training=False):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_fn(file_names, mode=tf.estimator.ModeKeys.EVAL, batch_size=1):\n",
    "  def _input_fn():\n",
    "    label_bytes = 1\n",
    "    image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    dataset = tf.data.FixedLengthRecordDataset(filenames=file_names,\n",
    "                                               record_bytes=record_bytes)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    if is_training:\n",
    "      buffer_size = batch_size * 2 + 1\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.map(lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "    # Batch results by up to batch_size, and then fetch the tuple from the\n",
    "    # iterator.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'images': _NumericColumn(key='images', shape=(32, 32, 3), default_value=None, dtype=tf.float32, normalizer_fn=None)}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measures the sparsity of activations.\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "  # session. This helps the clarity of presentation on tensorboard.\n",
    "  tensor_name = re.sub('%s_[0-9]*/' % FLAGS.tower_name, '', x.op.name)\n",
    "  tf.summary.histogram(tensor_name + '/activations', x)\n",
    "  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "  \n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "    \n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "  \n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    pool2_shape = pool2.get_shape()\n",
    "    dim = pool2_shape[1] * pool2_shape[2] * pool2_shape[3]\n",
    "    reshape = tf.reshape(pool2, [-1, dim])\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "    logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(logits)\n",
    "\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, labels):\n",
    "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "    logits=logits, onehot_labels=labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  loss = cross_entropy + FLAGS.weight_decay * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_op(loss, params, mode):\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # Scale the learning rate linearly with the batch size. When the batch size\n",
    "    # is 128, the learning rate should be 0.1.\n",
    "    initial_learning_rate = 0.1 * params.batch_size / 128\n",
    "    batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / params.batch_size\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "    values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "    learning_rate = tf.train.piecewise_constant(\n",
    "        tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=FLAGS.momentum)\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step)\n",
    "  else:\n",
    "    train_op = None\n",
    "    \n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(predictions, labels):\n",
    "  # Calculate accuracy\n",
    "  accuracy = tf.metrics.accuracy(predictions['classes'],\n",
    "                                 tf.argmax(labels, axis=1))\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_accuracy')\n",
    "  tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "  \n",
    "  return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(run_config, hparams):\n",
    "  def _model_fn(features, labels, mode, params):\n",
    "    # Create the input layers from the features\n",
    "    feature_columns = list(get_feature_columns().values())\n",
    "\n",
    "    images = tf.feature_column.input_layer(\n",
    "      features=features, feature_columns=feature_columns)\n",
    "\n",
    "    images = tf.reshape(\n",
    "      images, shape=(-1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "\n",
    "    # Calculate logits through CNN\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = {\n",
    "      'classes': tf.argmax(logits, axis=1),\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "\n",
    "    # Provide an estimator spec for `ModeKeys.PREDICT`\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      export_outputs = {\n",
    "        'predictions': tf.estimator.export.PredictOutput(predictions)\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                        predictions=predictions,\n",
    "                                        export_outputs=export_outputs)\n",
    "\n",
    "    loss = get_loss(logits=logits, labels=labels)\n",
    "    train_op = get_train_op(loss=loss, mode=mode, params=params)\n",
    "    metrics = get_metrics(predictions=predictions, labels=labels)\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=metrics)\n",
    "\n",
    "  return tf.estimator.Estimator(model_fn=_model_fn,\n",
    "                                params=hparams,\n",
    "                                config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "\n",
    "  receiver_tensor = {'images': tf.placeholder(\n",
    "    shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH], dtype=tf.float32)}\n",
    "  features = {'images': tf.map_fn(preprocess_image, receiver_tensor['images'])}\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train, Evaluate and Export a ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'trained_models/{}'.format(FLAGS.model_name)\n",
    "train_data_files = ['data/cifar-10-batches-bin/data_batch_{}.bin'.format(i) for i in range(1,5)]\n",
    "valid_data_files = ['data/cifar-10-batches-bin/data_batch_5.bin']\n",
    "test_data_files = ['data/cifar-10-batches-bin/test_batch.bin']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 19851211, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe5129cd290>, '_model_dir': 'trained_models/cnn-model-01', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\n"
     ]
    }
   ],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "  batch_size=FLAGS.batch_size,\n",
    "  max_steps=FLAGS.max_steps,\n",
    ")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "  save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "  tf_random_seed=FLAGS.tf_random_seed,\n",
    "  model_dir=model_dir\n",
    ")\n",
    "\n",
    "estimator = get_estimator(run_config, hparams)\n",
    "\n",
    "# There is another Exporter named FinalExporter\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "  name='Servo',\n",
    "  serving_input_receiver_fn=serving_input_fn,\n",
    "  assets_extra=None,\n",
    "  as_text=False,\n",
    "  exports_to_keep=5)\n",
    "\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn=generate_input_fn(file_names=train_data_files,\n",
    "                             mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                             batch_size=hparams.batch_size),\n",
    "  max_steps=FLAGS.max_steps)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn=generate_input_fn(file_names=valid_data_files,\n",
    "                             mode=tf.estimator.ModeKeys.EVAL,\n",
    "                             batch_size=hparams.batch_size),\n",
    "  steps=FLAGS.eval_steps, exporters=exporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n",
      "INFO:tensorflow:Running training and evaluation locally (non-distributed).\n",
      "INFO:tensorflow:Start train and evaluate loop. The evaluate will happen after 600 secs (eval_spec.throttle_secs) or training is finished.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.53278, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 101 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.00962\n",
      "INFO:tensorflow:loss = 2.60937, step = 101 (99.048 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 201 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 0.997318\n",
      "INFO:tensorflow:loss = 2.52636, step = 201 (100.275 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 301 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.01459\n",
      "INFO:tensorflow:loss = 2.39819, step = 301 (98.556 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 401 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.02128\n",
      "INFO:tensorflow:loss = 2.33593, step = 401 (97.931 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 501 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.03921\n",
      "INFO:tensorflow:loss = 2.28149, step = 501 (96.221 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 601 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.01197\n",
      "INFO:tensorflow:loss = 2.2136, step = 601 (98.813 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 608 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.05716.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-20-02:30:00\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-608\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-20-02:30:30\n",
      "INFO:tensorflow:Saving dict for global step 608: accuracy = 0.3254, global_step = 608, loss = 2.29749\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-608\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/cnn-model-01/export/Servo/temp-1513737030/saved_model.pb\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-608\n",
      "INFO:tensorflow:Saving checkpoints for 609 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:loss = 2.18787, step = 609\n",
      "INFO:tensorflow:Saving checkpoints for 709 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.1747\n",
      "INFO:tensorflow:loss = 2.22803, step = 709 (85.130 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 809 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.92176\n",
      "INFO:tensorflow:loss = 2.05951, step = 809 (52.035 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 909 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:global_step/sec: 1.9155\n",
      "INFO:tensorflow:loss = 2.04767, step = 909 (52.206 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 2.10685.\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-20-02:34:31\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Evaluation [51/100]\n",
      "INFO:tensorflow:Evaluation [52/100]\n",
      "INFO:tensorflow:Evaluation [53/100]\n",
      "INFO:tensorflow:Evaluation [54/100]\n",
      "INFO:tensorflow:Evaluation [55/100]\n",
      "INFO:tensorflow:Evaluation [56/100]\n",
      "INFO:tensorflow:Evaluation [57/100]\n",
      "INFO:tensorflow:Evaluation [58/100]\n",
      "INFO:tensorflow:Evaluation [59/100]\n",
      "INFO:tensorflow:Evaluation [60/100]\n",
      "INFO:tensorflow:Evaluation [61/100]\n",
      "INFO:tensorflow:Evaluation [62/100]\n",
      "INFO:tensorflow:Evaluation [63/100]\n",
      "INFO:tensorflow:Evaluation [64/100]\n",
      "INFO:tensorflow:Evaluation [65/100]\n",
      "INFO:tensorflow:Evaluation [66/100]\n",
      "INFO:tensorflow:Evaluation [67/100]\n",
      "INFO:tensorflow:Evaluation [68/100]\n",
      "INFO:tensorflow:Evaluation [69/100]\n",
      "INFO:tensorflow:Evaluation [70/100]\n",
      "INFO:tensorflow:Evaluation [71/100]\n",
      "INFO:tensorflow:Evaluation [72/100]\n",
      "INFO:tensorflow:Evaluation [73/100]\n",
      "INFO:tensorflow:Evaluation [74/100]\n",
      "INFO:tensorflow:Evaluation [75/100]\n",
      "INFO:tensorflow:Evaluation [76/100]\n",
      "INFO:tensorflow:Evaluation [77/100]\n",
      "INFO:tensorflow:Evaluation [78/100]\n",
      "INFO:tensorflow:Evaluation [79/100]\n",
      "INFO:tensorflow:Evaluation [80/100]\n",
      "INFO:tensorflow:Evaluation [81/100]\n",
      "INFO:tensorflow:Evaluation [82/100]\n",
      "INFO:tensorflow:Evaluation [83/100]\n",
      "INFO:tensorflow:Evaluation [84/100]\n",
      "INFO:tensorflow:Evaluation [85/100]\n",
      "INFO:tensorflow:Evaluation [86/100]\n",
      "INFO:tensorflow:Evaluation [87/100]\n",
      "INFO:tensorflow:Evaluation [88/100]\n",
      "INFO:tensorflow:Evaluation [89/100]\n",
      "INFO:tensorflow:Evaluation [90/100]\n",
      "INFO:tensorflow:Evaluation [91/100]\n",
      "INFO:tensorflow:Evaluation [92/100]\n",
      "INFO:tensorflow:Evaluation [93/100]\n",
      "INFO:tensorflow:Evaluation [94/100]\n",
      "INFO:tensorflow:Evaluation [95/100]\n",
      "INFO:tensorflow:Evaluation [96/100]\n",
      "INFO:tensorflow:Evaluation [97/100]\n",
      "INFO:tensorflow:Evaluation [98/100]\n",
      "INFO:tensorflow:Evaluation [99/100]\n",
      "INFO:tensorflow:Evaluation [100/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-20-02:34:47\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.3925, global_step = 1000, loss = 2.05727\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-1000\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/cnn-model-01/export/Servo/temp-1513737287/saved_model.pb\n"
     ]
    }
   ],
   "source": [
    "if not FLAGS.use_checkpoint:\n",
    "  print(\"Removing previous artifacts...\")\n",
    "  shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate with Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_save_checkpoints_secs': None, '_session_config': None, '_keep_checkpoint_max': 5, '_tf_random_seed': 19851211, '_task_type': 'worker', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fe5129cd290>, '_model_dir': 'trained_models/cnn-model-01', '_num_worker_replicas': 1, '_task_id': 0, '_log_step_count_steps': 100, '_master': '', '_save_checkpoints_steps': 100, '_keep_checkpoint_every_n_hours': 10000, '_service': None, '_save_summary_steps': 100, '_num_ps_replicas': 0}\n",
      "INFO:tensorflow:Starting evaluation at 2017-12-20-02:34:48\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-1000\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-12-20-02:34:49\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.404, global_step = 1000, loss = 2.02732\n",
      "{'loss': 2.0273185, 'global_step': 1000, 'accuracy': 0.40400001}\n"
     ]
    }
   ],
   "source": [
    "test_input_fn = generate_input_fn(file_names=test_data_files,\n",
    "                                  mode=tf.estimator.ModeKeys.EVAL,\n",
    "                                  batch_size=1000)\n",
    "estimator = get_estimator(run_config, hparams)\n",
    "print(estimator.evaluate(input_fn=test_input_fn, steps=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluate with an exported model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/export/Servo/1513737287/variables/variables\n"
     ]
    }
   ],
   "source": [
    "export_dir = model_dir + '/export/Servo/'\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir,\n",
    "  signature_def_key='predictions')\n",
    "\n",
    "N = 1000\n",
    "labels = []\n",
    "images = []\n",
    "\n",
    "for i in range(N):\n",
    "  result = extract_data(i, filepath='data/cifar-10-batches-bin/test_batch.bin')\n",
    "  images.append(result['image'])\n",
    "  labels.append(result['label'][0])\n",
    "\n",
    "output = predictor_fn({'images': images})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40400000000000003"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([a==r for a, r in zip(labels, output['classes'])]) / float(N)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
