{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook describes how to implement distributed tensorflow code.\n",
    "\n",
    "Content of this notebook is shown below.\n",
    "\n",
    "0. Prepare CIFAR-10 Dataset (TFRecords Format)\n",
    "1. Define dataset and global constants\n",
    "2. Define data input function\n",
    "3. Define feature columns\n",
    "4. Instantiate an Estimator\n",
    "5. Train, evaluate and export ML models\n",
    "6. Evaluate with Estimator\n",
    "7. Prediction with Exported Model\n",
    "8. Distributed Training with Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Prepare CIFAR-10 Dataset (TFRecords Format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import tarfile\n",
    "import tensorflow as tf\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CIFAR_FILENAME = 'cifar-10-python.tar.gz'\n",
    "CIFAR_DOWNLOAD_URL = 'http://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME\n",
    "CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _download_and_extract(data_dir):\n",
    "  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir, CIFAR_DOWNLOAD_URL)\n",
    "  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME), 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_file_names():\n",
    "  \"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n",
    "  file_names = {}\n",
    "  file_names['train'] = ['data_batch_%d' % i for i in xrange(1, 5)]\n",
    "  file_names['validation'] = ['data_batch_5']\n",
    "  file_names['eval'] = ['test_batch']\n",
    "  return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_pickle_from_file(filename):\n",
    "  with tf.gfile.Open(filename, 'r') as f:\n",
    "    data_dict = cPickle.load(f)\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _convert_to_tfrecord(input_files, output_file):\n",
    "  \"\"\"Converts a file to TFRecords.\"\"\"\n",
    "  print('Generating %s' % output_file)\n",
    "  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n",
    "    for input_file in input_files:\n",
    "      data_dict = _read_pickle_from_file(input_file)\n",
    "      data = data_dict['data']\n",
    "      labels =  data_dict['labels']\n",
    "      num_entries_in_batch = len(labels)\n",
    "      for i in range(num_entries_in_batch):\n",
    "        example = tf.train.Example(features=tf.train.Features(\n",
    "          feature={\n",
    "            'image': _bytes_feature(data[i].tobytes()),\n",
    "            'label': _int64_feature(labels[i])\n",
    "          }))\n",
    "        record_writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecords_files(data_dir='cifar-10'):\n",
    "  _download_and_extract(data_dir)\n",
    "  file_names = _get_file_names()\n",
    "  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n",
    "\n",
    "  for mode, files in file_names.items():\n",
    "    input_files = [os.path.join(input_dir, f) for f in files]\n",
    "    output_file = os.path.join(data_dir, mode+'.tfrecords')\n",
    "    try:\n",
    "      os.remove(output_file)\n",
    "    except OSError:\n",
    "      pass\n",
    "    # Convert to tf.train.Example and write to TFRecords.\n",
    "    _convert_to_tfrecord(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tfrecords_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLAGS():\n",
    "  pass\n",
    "\n",
    "FLAGS.batch_size = 200\n",
    "FLAGS.max_steps = 1000\n",
    "FLAGS.eval_steps = 100\n",
    "FLAGS.save_checkpoints_steps = 100\n",
    "FLAGS.tf_random_seed = 19851211\n",
    "FLAGS.model_name = 'cnn-model-02'\n",
    "\n",
    "# We use a weight decay of 0.0002, which performs better than the 0.0001 that\n",
    "# was originally suggested.\n",
    "FLAGS.weight_decay = 2e-4\n",
    "FLAGS.momentum = 0.9\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "FLAGS.tower_name = 'tower'\n",
    "FLAGS.use_checkpoint = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Data Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_record(serialized_example):\n",
    "  features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      'image': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  \n",
    "  image = tf.decode_raw(features['image'], tf.uint8)\n",
    "  image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])\n",
    "  image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n",
    "  \n",
    "  label = tf.cast(features['label'], tf.int32)\n",
    "  label = tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image, is_training=False):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_input_fn(file_names, mode=tf.estimator.ModeKeys.EVAL, batch_size=1):\n",
    "  def _input_fn():\n",
    "    dataset = tf.data.TFRecordDataset(filenames=file_names)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    if is_training:\n",
    "      buffer_size = batch_size * 2 + 1\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    # Transformation\n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.map(\n",
    "      lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "    images, labels = dataset.make_one_shot_iterator().get_next()\n",
    "\n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Instantiate an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measures the sparsity of activations.\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "  # session. This helps the clarity of presentation on tensorboard.\n",
    "  tensor_name = re.sub('%s_[0-9]*/' % FLAGS.tower_name, '', x.op.name)\n",
    "  tf.summary.histogram(tensor_name + '/activations', x)\n",
    "  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "  \n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "    \n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "  \n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    pool2_shape = pool2.get_shape()\n",
    "    dim = pool2_shape[1] * pool2_shape[2] * pool2_shape[3]\n",
    "    reshape = tf.reshape(pool2, [-1, dim])\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "    logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(logits)\n",
    "\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(logits, labels):\n",
    "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "    logits=logits, onehot_labels=labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  loss = cross_entropy + FLAGS.weight_decay * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_op(loss, params, mode):\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # Scale the learning rate linearly with the batch size. When the batch size\n",
    "    # is 128, the learning rate should be 0.1.\n",
    "    initial_learning_rate = 0.1 * params.batch_size / 128\n",
    "    batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / params.batch_size\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "    values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "    learning_rate = tf.train.piecewise_constant(\n",
    "        tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=FLAGS.momentum)\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step)\n",
    "  else:\n",
    "    train_op = None\n",
    "    \n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(predictions, labels):\n",
    "  # Calculate accuracy\n",
    "  accuracy = tf.metrics.accuracy(predictions['classes'],\n",
    "                                 tf.argmax(labels, axis=1))\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_accuracy')\n",
    "  tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "  \n",
    "  return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_estimator(run_config, hparams):\n",
    "  def _model_fn(features, labels, mode, params):\n",
    "    # Create the input layers from the features\n",
    "    feature_columns = list(get_feature_columns().values())\n",
    "\n",
    "    images = tf.feature_column.input_layer(\n",
    "      features=features, feature_columns=feature_columns)\n",
    "\n",
    "    images = tf.reshape(\n",
    "      images, shape=(-1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "\n",
    "    # Calculate logits through CNN\n",
    "    logits = inference(images)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = {\n",
    "      'classes': tf.argmax(logits, axis=1),\n",
    "      'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "    }\n",
    "\n",
    "    # Provide an estimator spec for `ModeKeys.PREDICT`\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "      export_outputs = {\n",
    "        'predictions': tf.estimator.export.PredictOutput(predictions)\n",
    "      }\n",
    "      return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                        predictions=predictions,\n",
    "                                        export_outputs=export_outputs)\n",
    "\n",
    "    loss = get_loss(logits=logits, labels=labels)\n",
    "    train_op = get_train_op(loss=loss, mode=mode, params=params)\n",
    "    metrics = get_metrics(predictions=predictions, labels=labels)\n",
    "\n",
    "    # Return EstimatorSpec\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "      mode=mode,\n",
    "      predictions=predictions,\n",
    "      loss=loss,\n",
    "      train_op=train_op,\n",
    "      eval_metric_ops=metrics)\n",
    "\n",
    "  return tf.estimator.Estimator(model_fn=_model_fn,\n",
    "                                params=hparams,\n",
    "                                config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "  receiver_tensor = {'images': tf.placeholder(\n",
    "    shape=[None, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH], dtype=tf.float32)}\n",
    "  features = {'images': tf.map_fn(preprocess_image, receiver_tensor['images'])}\n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train, Evaluate and Export a ML Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'trained_models/{}'.format(FLAGS.model_name)\n",
    "train_data_files = ['cifar-10/train.tfrecords']\n",
    "valid_data_files = ['cifar-10/validation.tfrecords']\n",
    "test_data_files = ['cifar-10/eval.tfrecords']\n",
    "train_data_files = ['gs://yaboo-sandbox-poc-for-lg/cifar-10/train.tfrecords']\n",
    "valid_data_files = ['gs://yaboo-sandbox-poc-for-lg/cifar-10/validation.tfrecords']\n",
    "test_data_files = ['gs://yaboo-sandbox-poc-for-lg/cifar-10/eval.tfrecords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = tf.contrib.training.HParams(\n",
    "  batch_size=FLAGS.batch_size,\n",
    "  max_steps=FLAGS.max_steps,\n",
    ")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "  save_checkpoints_steps=FLAGS.save_checkpoints_steps,\n",
    "  tf_random_seed=FLAGS.tf_random_seed,\n",
    "  model_dir=model_dir\n",
    ")\n",
    "\n",
    "estimator = get_estimator(run_config, hparams)\n",
    "\n",
    "# There is another Exporter named FinalExporter\n",
    "exporter = tf.estimator.LatestExporter(\n",
    "  name='Servo',\n",
    "  serving_input_receiver_fn=serving_input_fn,\n",
    "  assets_extra=None,\n",
    "  as_text=False,\n",
    "  exports_to_keep=5)\n",
    "\n",
    "\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn=generate_input_fn(file_names=train_data_files,\n",
    "                             mode=tf.estimator.ModeKeys.TRAIN,\n",
    "                             batch_size=hparams.batch_size),\n",
    "  max_steps=FLAGS.max_steps)\n",
    "\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn=generate_input_fn(file_names=valid_data_files,\n",
    "                             mode=tf.estimator.ModeKeys.EVAL,\n",
    "                             batch_size=hparams.batch_size),\n",
    "  steps=FLAGS.eval_steps, exporters=exporter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not FLAGS.use_checkpoint:\n",
    "  print(\"Removing previous artifacts...\")\n",
    "  shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate with Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input_fn = generate_input_fn(file_names=test_data_files,\n",
    "                                  mode=tf.estimator.ModeKeys.EVAL,\n",
    "                                  batch_size=1000)\n",
    "estimator = get_estimator(run_config, hparams)\n",
    "print(estimator.evaluate(input_fn=test_input_fn, steps=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Prediction with Exported Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_dir = model_dir + '/export/Servo/'\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir,\n",
    "  signature_def_key='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "\n",
    "data_dict = _read_pickle_from_file('cifar-10/cifar-10-batches-py/test_batch')\n",
    "\n",
    "N = 1000\n",
    "images = data_dict['data'][:N].reshape([N, 3, 32, 32]).transpose([0, 2, 3, 1])\n",
    "labels = data_dict['labels'][:N]\n",
    "\n",
    "output = predictor_fn({'images': images})\n",
    "accuracy = numpy.sum(\n",
    "  [ans==ret for ans, ret in zip(labels, output['classes'])]) / float(N)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Distributed Training with Cloud ML Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Set environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "PROJECT = 'project-id' # REPLACE WITH YOUR PROJECT ID\n",
    "BUCKET = 'bucket-name' # REPLACE WITH YOUR BUCKET NAME\n",
    "REGION = 'bucket-region' # REPLACE WITH YOUR BUCKET REGION e.g. us-central1\n",
    "\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set permission to BUCKET (NOTE: Create bucket beforehand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PROJECT_ID=$PROJECT\n",
    "AUTH_TOKEN=$(gcloud auth print-access-token)\n",
    "\n",
    "SVC_ACCOUNT=$(curl -X GET -H \"Content-Type: application/json\" \\\n",
    "    -H \"Authorization: Bearer $AUTH_TOKEN\" \\\n",
    "    https://ml.googleapis.com/v1/projects/${PROJECT_ID}:getConfig \\\n",
    "    | python -c \"import json; import sys; response = json.load(sys.stdin); \\\n",
    "    print response['serviceAccount']\")\n",
    "\n",
    "echo \"Authorizing the Cloud ML Service account $SVC_ACCOUNT to access files in $BUCKET\"\n",
    "gsutil -m defacl ch -u $SVC_ACCOUNT:R gs://$BUCKET\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:R -r gs://$BUCKET  # error message (if bucket is empty) can be ignored\n",
    "gsutil -m acl ch -u $SVC_ACCOUNT:W gs://$BUCKET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Copy TFRecords files to GCS BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo ${BUCKET}\n",
    "gsutil -m rm -rf gs://${BUCKET}/cifar-10\n",
    "gsutil -m cp cifar-10/*.tfrecords gs://${BUCKET}/cifar-10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run distributed training with Cloud MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "OUTDIR=gs://$BUCKET/trained_models\n",
    "JOBNAME=sm_$(date -u +%y%m%d_%H%M%S)\n",
    "echo $OUTDIR $REGION $JOBNAME\n",
    "\n",
    "gsutil -m rm -rf $OUTDIR\n",
    "gcloud ml-engine jobs submit training $JOBNAME \\\n",
    "   --region=$REGION \\\n",
    "   --module-name=cnn-model-02.task \\\n",
    "   --package-path=\"$(pwd)/trainer/cnn-model-02\" \\\n",
    "   --job-dir=$OUTDIR \\\n",
    "   --staging-bucket=gs://$BUCKET \\\n",
    "   --scale-tier=STANDARD_1 \\\n",
    "   --runtime-version=1.4 \\\n",
    "   -- \\\n",
    "   --bucket_name=$BUCKET \\\n",
    "   --train_data_pattern=cifar-10/train*.tfrecords \\\n",
    "   --eval_data_pattern=cifar-10/eval*.tfrecords  \\\n",
    "   --output_dir=$OUTDIR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
