{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Download CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "from six.moves import urllib\n",
    "\n",
    "def maybe_download_and_extract(\n",
    "  dest_directory='data',\n",
    "  data_url='http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'):\n",
    "  \n",
    "  \"\"\"Download and extract the tarball from Alex's website.\"\"\"\n",
    "  dest_directory = dest_directory\n",
    "  if not os.path.exists(dest_directory):\n",
    "    os.makedirs(dest_directory)\n",
    "  filename = data_url.split('/')[-1]\n",
    "  filepath = os.path.join(dest_directory, filename)\n",
    "  if not os.path.exists(filepath):\n",
    "    def _progress(count, block_size, total_size):\n",
    "      sys.stdout.write('\\r>> Downloading %s %.1f%%' % (\n",
    "          filename, float(count * block_size) / float(total_size) * 100.0))\n",
    "      sys.stdout.flush()\n",
    "    filepath, _ = urllib.request.urlretrieve(DATA_URL, filepath, _progress)\n",
    "    print()\n",
    "    statinfo = os.stat(filepath)\n",
    "    print('Successfully downloaded', filename, statinfo.st_size, 'bytes.')\n",
    "  extracted_dir_path = os.path.join(dest_directory, 'cifar-10-batches-bin')\n",
    "  if not os.path.exists(extracted_dir_path):\n",
    "    tarfile.open(filepath, 'r:gz').extractall(dest_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-binary.tar.gz 15.1%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Downloading cifar-10-binary.tar.gz 100.0%()\n",
      "('Successfully downloaded', 'cifar-10-binary.tar.gz', 170052171, 'bytes.')\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = 'data'\n",
    "DATA_URL = 'http://www.cs.toronto.edu/~kriz/cifar-10-binary.tar.gz'\n",
    "maybe_download_and_extract(DATA_DIR, DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b. Exploring CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def extract_data(index=0, filepath='data/cifar-10-batches-bin/data_batch_5.bin'):\n",
    "  bytestream = open(filepath, mode='rb')\n",
    "\n",
    "  label_bytes_length = 1\n",
    "  image_bytes_length = (32 ** 2) * 3\n",
    "  record_bytes_length = label_bytes_length + image_bytes_length\n",
    "\n",
    "  bytestream.seek(record_bytes_length * index, 0)\n",
    "  label_bytes = bytestream.read(label_bytes_length)\n",
    "  image_bytes = bytestream.read(image_bytes_length)\n",
    "\n",
    "  label = np.frombuffer(label_bytes, dtype=np.uint8)  \n",
    "  image = np.frombuffer(image_bytes, dtype=np.uint8)\n",
    "  \n",
    "  image = np.reshape(image, [3, 32, 32])\n",
    "  image = np.transpose(image, [1, 2, 0])\n",
    "  image = image.astype(np.float32)\n",
    "  \n",
    "  result = {\n",
    "    'image': image,\n",
    "    'label': label,\n",
    "  }\n",
    "  bytestream.close()\n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f9b07341c90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9b44477fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "result = extract_data(np.random.randint(1000))\n",
    "plt.imshow(result['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata** and **global constants**\n",
    "2. Define **data input function** to read the data from the source + **apply pre-processing**\n",
    "3. Create TF **feature columns** based on metadata + **extended feature columns**\n",
    "4. Instantiate a **model function** with the required **feature columns, EstimatorSpecs, & parameters**\n",
    "5. Define a **serving function**\n",
    "6. Run **Experiment** by supplying training and validation data, as well as required parameters\n",
    "7. **Evaluate** the model using test data\n",
    "8. Perform **predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn import make_export_strategy\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_files = ['data/cifar-10-batches-bin/data_batch_{}.bin'.format(i) for i in range(1,5)]\n",
    "valid_data_files = ['data/cifar-10-batches-bin/data_batch_5.bin']\n",
    "test_data_files = ['data/cifar-10-batches-bin/test_batch.bin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define dataset metadata and global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "# We use a weight decay of 0.0002, which performs better than the 0.0001 that\n",
    "# was originally suggested.\n",
    "WEIGHT_DECAY = 2e-4\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Global constants describing model behaviors\n",
    "MODEL_NAME = 'cnn-model-01'\n",
    "USE_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. parsing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_record(raw_record):\n",
    "  # Every record consists of a label followed by the image, with a fixed number\n",
    "  # of bytes for each.\n",
    "  label_bytes = 1\n",
    "  image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "  record_bytes = label_bytes + image_bytes\n",
    "  \n",
    "  # Convert from a string to a vector of uint8 that is record_bytes long.\n",
    "  record_vector = tf.decode_raw(raw_record, tf.uint8)\n",
    "  \n",
    "  # The first byte represents the label, which we convert from uint8 to int32\n",
    "  # and then to one-hot.\n",
    "  label = tf.cast(record_vector[0], tf.int32)\n",
    "  label = tf.one_hot(label, NUM_CLASSES)\n",
    "  \n",
    "  # The remaining bytes after the label represent the image, which we reshape\n",
    "  # from [depth * height * width] to [depth, height, width].\n",
    "  depth_major = tf.reshape(\n",
    "    record_vector[label_bytes:record_bytes], [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  \n",
    "  # Convert from [depth, height, width] to [height, width, depth], and cast as\n",
    "  # float32.\n",
    "  image = tf.cast(tf.transpose(depth_major, [1, 2, 0]), tf.float32)\n",
    "  \n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. preprocessing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, is_training=False):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(file_names,\n",
    "                      mode=tf.estimator.ModeKeys.EVAL,\n",
    "                      num_epochs=None,\n",
    "                      batch_size=1):\n",
    "\n",
    "  def _input_fn():\n",
    "    label_bytes = 1\n",
    "    image_bytes = IMAGE_HEIGHT * IMAGE_WIDTH * IMAGE_DEPTH\n",
    "    record_bytes = label_bytes + image_bytes\n",
    "    dataset = tf.data.FixedLengthRecordDataset(filenames=file_names,\n",
    "                                               record_bytes=record_bytes)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    if is_training:\n",
    "      buffer_size = batch_size * 2 + 1\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.map(lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "    # Batch results by up to batch_size, and then fetch the tuple from the\n",
    "    # iterator.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Columns: {'images': _NumericColumn(key='images', shape=(32, 32, 3), default_value=None, dtype=tf.float32, normalizer_fn=None)}\n"
     ]
    }
   ],
   "source": [
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measures the sparsity of activations.\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "  # session. This helps the clarity of presentation on tensorboard.\n",
    "  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "  tf.summary.histogram(tensor_name + '/activations', x)\n",
    "  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "  \n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "    \n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "  \n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    pool2_shape = pool2.get_shape()\n",
    "    dim = pool2_shape[1] * pool2_shape[2] * pool2_shape[3]\n",
    "    reshape = tf.reshape(pool2, [-1, dim])\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "    logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(logits)\n",
    "\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(logits, labels):\n",
    "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "    logits=logits, onehot_labels=labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  loss = cross_entropy + WEIGHT_DECAY * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_op(loss, params, mode):\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # Scale the learning rate linearly with the batch size. When the batch size\n",
    "    # is 128, the learning rate should be 0.1.\n",
    "    initial_learning_rate = 0.1 * params.batch_size / 128\n",
    "    batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / params.batch_size\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "    values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "    learning_rate = tf.train.piecewise_constant(\n",
    "        tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=MOMENTUM)\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step)\n",
    "  else:\n",
    "    train_op = None\n",
    "    \n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(predictions, labels):\n",
    "  # Calculate accuracy\n",
    "  accuracy = tf.metrics.accuracy(predictions['classes'],\n",
    "                                 tf.argmax(labels, axis=1))\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_accuracy')\n",
    "  tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "  \n",
    "  return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  # Create the input layers from the features\n",
    "  feature_columns = list(get_feature_columns().values())\n",
    "  \n",
    "  images = tf.feature_column.input_layer(\n",
    "    features=features, feature_columns=feature_columns)\n",
    "  \n",
    "  images = tf.reshape(\n",
    "    images, shape=(-1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "\n",
    "  # Calculate logits through CNN\n",
    "  logits = inference(images)\n",
    "\n",
    "  # Get predictions\n",
    "  predictions = {\n",
    "    'classes': tf.argmax(logits, axis=1),\n",
    "    'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "  }\n",
    "\n",
    "  # Provide an estimator spec for `ModeKeys.PREDICT`\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    export_outputs = {\n",
    "      'predictions': tf.estimator.export.PredictOutput(predictions)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions,\n",
    "                                      export_outputs=export_outputs)\n",
    "\n",
    "  loss = get_loss(logits=logits, labels=labels)\n",
    "  train_op = get_train_op(loss=loss, mode=mode, params=params)\n",
    "  metrics = get_metrics(predictions=predictions, labels=labels)\n",
    "  \n",
    "  # Return EstimatorSpec\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    predictions=predictions,\n",
    "    loss=loss,\n",
    "    train_op=train_op,\n",
    "    eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "  return tf.estimator.Estimator(\n",
    "      model_fn=model_fn,\n",
    "      params=hparams,\n",
    "      config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Define Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_experiment_fn(**experiment_args):\n",
    "  def _experiment_fn(run_config, hparams):\n",
    "    return tf.contrib.learn.Experiment(\n",
    "      estimator=create_estimator(run_config, hparams),\n",
    "      train_input_fn=generate_input_fn(file_names=train_data_files,\n",
    "                                       mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "                                       num_epochs=hparams.num_epochs,\n",
    "                                       batch_size=hparams.batch_size),\n",
    "      eval_input_fn=generate_input_fn(file_names=valid_data_files,\n",
    "                                      mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                      num_epochs=hparams.num_epochs,\n",
    "                                      batch_size=hparams.batch_size),\n",
    "      **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('batch_size', 200), ('num_epochs', 1)]\n",
      "('Model Directory:', 'trained_models/cnn-model-01')\n",
      "\n",
      "('Dataset Size:', 50000)\n",
      "('Batch Size:', 200)\n",
      "('Steps per Epoch:', 250)\n",
      "('Total Steps:', 250)\n",
      "('Required Evaluation Steps:', 1)\n",
      "('That is 1 evaluation step after each', 1, ' epochs')\n",
      "('Save Checkpoint After', 250, 'steps')\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 200\n",
    "TRAIN_SIZE = 50000\n",
    "NUM_EVAL = 1\n",
    "CHECKPOINT_STEPS = int((TRAIN_SIZE/BATCH_SIZE) * (NUM_EPOCHS/NUM_EVAL))\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "  num_epochs=NUM_EPOCHS,\n",
    "  batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "  save_checkpoints_steps=CHECKPOINT_STEPS,\n",
    "  tf_random_seed=19851211,\n",
    "  model_dir=model_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(hparams)\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"\")\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS)\n",
    "print(\"Required Evaluation Steps:\", NUM_EVAL) \n",
    "print(\"That is 1 evaluation step after each\",NUM_EPOCHS/NUM_EVAL,\" epochs\")\n",
    "print(\"Save Checkpoint After\",CHECKPOINT_STEPS,\"steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Define Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "\n",
    "  receiver_tensor = {'images': tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)}\n",
    "  features = {'images': tf.map_fn(preprocess_image, receiver_tensor['images'])}\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run Experiment via learn_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing previous artifacts...\n",
      "Experiment started at 06:55:05\n",
      ".......................................\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/cnn-model-01', '_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': 19851211, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9b4695ed10>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': 250, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\n",
      "WARNING:tensorflow:uid (from tensorflow.contrib.learn.python.learn.estimators.run_config) is experimental and may change or be removed at any time, and without warning.\n",
      "WARNING:tensorflow:From /usr/local/google/home/yaboo/Resources/anaconda2/lib/python2.7/site-packages/tensorflow/contrib/learn/python/learn/monitors.py:267: __init__ (from tensorflow.contrib.learn.python.learn.monitors) is deprecated and will be removed after 2016-12-05.\n",
      "Instructions for updating:\n",
      "Monitors are deprecated. Please use tf.train.SessionRunHook.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-27-06:55:07\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-1\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-27-06:55:15\n",
      "INFO:tensorflow:Saving dict for global step 1: accuracy = 0.1149, global_step = 1, loss = 2.54835\n",
      "INFO:tensorflow:Validation (step 1): loss = 2.54835, global_step = 1, accuracy = 0.1149\n",
      "INFO:tensorflow:loss = 2.53371, step = 1\n",
      "INFO:tensorflow:global_step/sec: 1.56036\n",
      "INFO:tensorflow:loss = 3.8808, step = 101 (55.426 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 200 into trained_models/cnn-model-01/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 3.78393.\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-27-06:57:02\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/100]\n",
      "INFO:tensorflow:Evaluation [2/100]\n",
      "INFO:tensorflow:Evaluation [3/100]\n",
      "INFO:tensorflow:Evaluation [4/100]\n",
      "INFO:tensorflow:Evaluation [5/100]\n",
      "INFO:tensorflow:Evaluation [6/100]\n",
      "INFO:tensorflow:Evaluation [7/100]\n",
      "INFO:tensorflow:Evaluation [8/100]\n",
      "INFO:tensorflow:Evaluation [9/100]\n",
      "INFO:tensorflow:Evaluation [10/100]\n",
      "INFO:tensorflow:Evaluation [11/100]\n",
      "INFO:tensorflow:Evaluation [12/100]\n",
      "INFO:tensorflow:Evaluation [13/100]\n",
      "INFO:tensorflow:Evaluation [14/100]\n",
      "INFO:tensorflow:Evaluation [15/100]\n",
      "INFO:tensorflow:Evaluation [16/100]\n",
      "INFO:tensorflow:Evaluation [17/100]\n",
      "INFO:tensorflow:Evaluation [18/100]\n",
      "INFO:tensorflow:Evaluation [19/100]\n",
      "INFO:tensorflow:Evaluation [20/100]\n",
      "INFO:tensorflow:Evaluation [21/100]\n",
      "INFO:tensorflow:Evaluation [22/100]\n",
      "INFO:tensorflow:Evaluation [23/100]\n",
      "INFO:tensorflow:Evaluation [24/100]\n",
      "INFO:tensorflow:Evaluation [25/100]\n",
      "INFO:tensorflow:Evaluation [26/100]\n",
      "INFO:tensorflow:Evaluation [27/100]\n",
      "INFO:tensorflow:Evaluation [28/100]\n",
      "INFO:tensorflow:Evaluation [29/100]\n",
      "INFO:tensorflow:Evaluation [30/100]\n",
      "INFO:tensorflow:Evaluation [31/100]\n",
      "INFO:tensorflow:Evaluation [32/100]\n",
      "INFO:tensorflow:Evaluation [33/100]\n",
      "INFO:tensorflow:Evaluation [34/100]\n",
      "INFO:tensorflow:Evaluation [35/100]\n",
      "INFO:tensorflow:Evaluation [36/100]\n",
      "INFO:tensorflow:Evaluation [37/100]\n",
      "INFO:tensorflow:Evaluation [38/100]\n",
      "INFO:tensorflow:Evaluation [39/100]\n",
      "INFO:tensorflow:Evaluation [40/100]\n",
      "INFO:tensorflow:Evaluation [41/100]\n",
      "INFO:tensorflow:Evaluation [42/100]\n",
      "INFO:tensorflow:Evaluation [43/100]\n",
      "INFO:tensorflow:Evaluation [44/100]\n",
      "INFO:tensorflow:Evaluation [45/100]\n",
      "INFO:tensorflow:Evaluation [46/100]\n",
      "INFO:tensorflow:Evaluation [47/100]\n",
      "INFO:tensorflow:Evaluation [48/100]\n",
      "INFO:tensorflow:Evaluation [49/100]\n",
      "INFO:tensorflow:Evaluation [50/100]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-27-06:57:11\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.1185, global_step = 200, loss = 3.77917\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-200\n",
      "INFO:tensorflow:Assets added to graph.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: trained_models/cnn-model-01/export/Servo/temp-1511765831/saved_model.pb\n",
      ".......................................\n",
      "Experiment finished at 06:57:11\n",
      "\n",
      "Experiment elapsed time: 126.273266 seconds\n"
     ]
    }
   ],
   "source": [
    "if not USE_CHECKPOINT:\n",
    "  print(\"Removing previous artifacts...\")\n",
    "  shutil.rmtree(model_dir, ignore_errors=True)\n",
    "else:\n",
    "  print(\"Resuming training...\")\n",
    "  \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow()\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\")\n",
    "\n",
    "learn_runner.run(\n",
    "  experiment_fn=generate_experiment_fn(\n",
    "    export_strategies=[make_export_strategy(\n",
    "        serving_input_fn,\n",
    "        exports_to_keep=1\n",
    "      )]\n",
    "  ),\n",
    "  schedule='train_and_evaluate',\n",
    "  run_config=run_config,\n",
    "  hparams=hparams\n",
    ")\n",
    "\n",
    "time_end = datetime.utcnow()\n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'trained_models/cnn-model-01', '_save_checkpoints_secs': None, '_num_ps_replicas': 0, '_keep_checkpoint_max': 5, '_session_config': None, '_tf_random_seed': 19851211, '_task_type': None, '_environment': 'local', '_is_chief': True, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f9b4695ed10>, '_tf_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 1.0\n",
      "}\n",
      ", '_num_worker_replicas': 0, '_task_id': 0, '_save_summary_steps': 100, '_save_checkpoints_steps': 250, '_evaluation_master': '', '_keep_checkpoint_every_n_hours': 10000, '_master': '', '_log_step_count_steps': 100}\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-27-06:57:11\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-27-06:57:12\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.136, global_step = 200, loss = 3.77639\n",
      "()\n",
      "######################################################################################\n",
      "# {'loss': 3.7763929, 'global_step': 200, 'accuracy': 0.13600001}\n",
      "######################################################################################\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-27-06:57:13\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-27-06:57:14\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.117, global_step = 200, loss = 3.7813\n",
      "()\n",
      "######################################################################################\n",
      "# {'loss': 3.7813025, 'global_step': 200, 'accuracy': 0.117}\n",
      "######################################################################################\n",
      "INFO:tensorflow:Starting evaluation at 2017-11-27-06:57:14\n",
      "INFO:tensorflow:Restoring parameters from trained_models/cnn-model-01/model.ckpt-200\n",
      "INFO:tensorflow:Evaluation [1/1]\n",
      "INFO:tensorflow:Finished evaluation at 2017-11-27-06:57:15\n",
      "INFO:tensorflow:Saving dict for global step 200: accuracy = 0.12, global_step = 200, loss = 3.77765\n",
      "()\n",
      "######################################################################################\n",
      "# {'loss': 3.7776546, 'global_step': 200, 'accuracy': 0.12}\n",
      "######################################################################################\n"
     ]
    }
   ],
   "source": [
    "train_size = 1000\n",
    "valid_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "train_input_fn = generate_input_fn(file_names=train_data_files,\n",
    "                                   mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "                                   num_epochs=None,\n",
    "                                   batch_size=train_size)\n",
    "\n",
    "valid_input_fn = generate_input_fn(file_names=valid_data_files,\n",
    "                                   mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                   num_epochs=None,\n",
    "                                   batch_size=valid_size)\n",
    "\n",
    "test_input_fn = generate_input_fn(file_names=test_data_files,\n",
    "                                  mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                  num_epochs=None,\n",
    "                                  batch_size=test_size)\n",
    "\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(train_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "valid_results = estimator.evaluate(input_fn=valid_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(valid_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(test_results))\n",
    "print(\"######################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "export_dir = model_dir + '/export/Servo/'\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "print(saved_model_dir)\n",
    "print('')\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir,\n",
    "  signature_def_key='predictions')\n",
    "\n",
    "N = 1000\n",
    "labels = []\n",
    "images = []\n",
    "\n",
    "for i in range(N):\n",
    "  result = extract_data(i)\n",
    "  images.append(result['image'])\n",
    "  labels.append(result['label'][0])\n",
    "\n",
    "output = predictor_fn(\n",
    "  {\n",
    "    'images': images,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum([a==r for a, r in zip(labels, output['classes'])]) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Using tf.keras for the Inference Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, mode):\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "  else:\n",
    "    tf.keras.backend.set_learning_phase(False)\n",
    "        \n",
    "  model = tf.keras.models.Sequential()\n",
    "  # Define input tensor in Keras world.\n",
    "  model.add(tf.keras.layers.InputLayer(input_tensor=images))\n",
    "    \n",
    "  # The first convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "  # The second convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  # The third convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "  # The fourth convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "  model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
    "    \n",
    "  logits = model.output\n",
    "    \n",
    "  return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Using gRPC to get prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from grpc.beta import implementations\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2\n",
    "\n",
    "def predict_with_grpc():\n",
    "  \"\"\"                                                                                                                                     \n",
    "  Note that you are running TensorFlow Serving with below commands.                                                                       \n",
    "  tensorflow_model_server --port=9000 --model_name=cnn-model-01 --model_base_path=./cnn-model-01                                          \n",
    "\n",
    "  In addition, make sure cnn-model-01 directory is organized as follows:                                                                  \n",
    "\n",
    "  cnn-model-01/:                                                                                                                          \n",
    "  {random_value}                                                                                                                              \n",
    "\n",
    "  cnn-model-01/{randam_value}/:                                                                                                           \n",
    "  saved_model.pb  variables                                                                                                               \n",
    "  \"\"\"\n",
    "  host = 'localhost'\n",
    "  port = '9000'\n",
    "  channel = implementations.insecure_channel(host, int(port))\n",
    "  stub = prediction_service_pb2.beta_create_PredictionService_stub(channel)\n",
    "\n",
    "  result = extract_data(0)\n",
    "  request = predict_pb2.PredictRequest()\n",
    "  request.model_spec.name = 'cnn-model-01'\n",
    "  request.model_spec.signature_name = 'predictions'\n",
    "  image = result['image']\n",
    "  label = result['label']\n",
    "  request.inputs['images'].CopyFrom(\n",
    "      tf.contrib.util.make_tensor_proto(image, shape=[1, 32, 32, 3]))\n",
    "\n",
    "  result_future = stub.Predict.future(request, 5.0)\n",
    "  print(result_future.result().outputs['classes'].int64_val)\n",
    "  print(result_future.result().outputs['probabilities'].float_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
