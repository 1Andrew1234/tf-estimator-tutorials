{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## a. Download CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle\n",
    "import os\n",
    "import tarfile\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CIFAR_FILENAME = 'cifar-10-python.tar.gz'\n",
    "CIFAR_DOWNLOAD_URL = 'http://www.cs.toronto.edu/~kriz/' + CIFAR_FILENAME\n",
    "CIFAR_LOCAL_FOLDER = 'cifar-10-batches-py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _download_and_extract(data_dir):\n",
    "  tf.contrib.learn.datasets.base.maybe_download(CIFAR_FILENAME, data_dir, CIFAR_DOWNLOAD_URL)\n",
    "  tarfile.open(os.path.join(data_dir, CIFAR_FILENAME), 'r:gz').extractall(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _get_file_names():\n",
    "  \"\"\"Returns the file names expected to exist in the input_dir.\"\"\"\n",
    "  file_names = {}\n",
    "  file_names['train'] = ['data_batch_%d' % i for i in xrange(1, 5)]\n",
    "  file_names['validation'] = ['data_batch_5']\n",
    "  file_names['eval'] = ['test_batch']\n",
    "  return file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_pickle_from_file(filename):\n",
    "  with tf.gfile.Open(filename, 'r') as f:\n",
    "    data_dict = cPickle.load(f)\n",
    "  return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _convert_to_tfrecord(input_files, output_file):\n",
    "  \"\"\"Converts a file to TFRecords.\"\"\"\n",
    "  print('Generating %s' % output_file)\n",
    "  with tf.python_io.TFRecordWriter(output_file) as record_writer:\n",
    "    for input_file in input_files:\n",
    "      data_dict = _read_pickle_from_file(input_file)\n",
    "      data = data_dict['data']\n",
    "      labels =  data_dict['labels']\n",
    "      num_entries_in_batch = len(labels)\n",
    "      for i in range(num_entries_in_batch):\n",
    "        example = tf.train.Example(features=tf.train.Features(\n",
    "          feature={\n",
    "            'image': _bytes_feature(data[i].tobytes()),\n",
    "            'label': _int64_feature(labels[i])\n",
    "          }))\n",
    "        record_writer.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _int64_feature(value):\n",
    "  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[str(value)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfrecord_files(data_dir='cifar-10'):\n",
    "  _download_and_extract(data_dir)\n",
    "  file_names = _get_file_names()\n",
    "  input_dir = os.path.join(data_dir, CIFAR_LOCAL_FOLDER)\n",
    "\n",
    "  for mode, files in file_names.items():\n",
    "    input_files = [os.path.join(input_dir, f) for f in files]\n",
    "    output_file = os.path.join(data_dir, mode+'.tfrecords')\n",
    "    try:\n",
    "      os.remove(output_file)\n",
    "    except OSError:\n",
    "      pass\n",
    "    # Convert to tf.train.Example and write to TFRecords.\n",
    "    _convert_to_tfrecord(input_files, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_tfrecord_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to use the TF Estimator APIs\n",
    "1. Define dataset **metadata** and **global constants**\n",
    "2. Define **data input function** to read the data from the source + **apply pre-processing**\n",
    "3. Create TF **feature columns** based on metadata + **extended feature columns**\n",
    "4. Instantiate a **model function** with the required **feature columns, EstimatorSpecs, & parameters**\n",
    "5. Define a **serving function**\n",
    "6. Run **Experiment** by supplying training and validation data, as well as required parameters\n",
    "7. **Evaluate** the model using test data\n",
    "8. Perform **predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "from datetime import datetime\n",
    "import re\n",
    "from tensorflow.python.feature_column import feature_column\n",
    "\n",
    "from tensorflow.contrib.learn import learn_runner\n",
    "from tensorflow.contrib.learn import make_export_strategy\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = [os.path.join('cifar-10', 'train.tfrecords')]\n",
    "valid_data_files = [os.path.join('cifar-10', 'validation.tfrecords')]\n",
    "test_data_files = [os.path.join('cifar-10', 'eval.tfrecords')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define dataset metadata and global constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Process images of this size. Note that this differs from the original CIFAR\n",
    "# image size of 32 x 32. If one alters this number, then the entire model\n",
    "# architecture will change and any model would need to be retrained.\n",
    "IMAGE_HEIGHT = 32\n",
    "IMAGE_WIDTH = 32\n",
    "IMAGE_DEPTH = 3\n",
    "\n",
    "# Global constants describing the CIFAR-10 data set.\n",
    "NUM_CLASSES = 10\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN = 50000\n",
    "NUM_EXAMPLES_PER_EPOCH_FOR_EVAL = 10000\n",
    "\n",
    "# If a model is trained with multiple GPUs, prefix all Op names with tower_name\n",
    "# to differentiate the operations. Note that this prefix is removed from the\n",
    "# names of the summaries when visualizing a model.\n",
    "TOWER_NAME = 'tower'\n",
    "\n",
    "# We use a weight decay of 0.0002, which performs better than the 0.0001 that\n",
    "# was originally suggested.\n",
    "WEIGHT_DECAY = 2e-4\n",
    "MOMENTUM = 0.9\n",
    "\n",
    "# Global constants describing model behaviors\n",
    "MODEL_NAME = 'cnn-model-01'\n",
    "USE_CHECKPOINT = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Data Input Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. parsing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_record(serialized_example):\n",
    "  features = tf.parse_single_example(\n",
    "    serialized_example,\n",
    "    features={\n",
    "      'image': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  \n",
    "  image = tf.decode_raw(features['image'], tf.uint8)\n",
    "  image.set_shape([IMAGE_DEPTH * IMAGE_HEIGHT * IMAGE_WIDTH])\n",
    "  image = tf.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  image = tf.cast(tf.transpose(image, [1, 2, 0]), tf.float32)\n",
    "  \n",
    "  label = tf.cast(features['label'], tf.int32)\n",
    "  label = tf.one_hot(label, NUM_CLASSES)\n",
    "\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. preprocessing CIFAR-10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, is_training=False):\n",
    "  \"\"\"Preprocess a single image of layout [height, width, depth].\"\"\"\n",
    "  if is_training:\n",
    "    # Resize the image to add four extra pixels on each side.\n",
    "    image = tf.image.resize_image_with_crop_or_pad(\n",
    "        image, IMAGE_HEIGHT + 8, IMAGE_WIDTH + 8)\n",
    "\n",
    "    # Randomly crop a [_HEIGHT, _WIDTH] section of the image.\n",
    "    image = tf.random_crop(image, [IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH])\n",
    "\n",
    "    # Randomly flip the image horizontally.\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "\n",
    "  # Subtract off the mean and divide by the variance of the pixels.\n",
    "  image = tf.image.per_image_standardization(image)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. data pipeline input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_input_fn(file_names,\n",
    "                      mode=tf.estimator.ModeKeys.EVAL,\n",
    "                      num_epochs=None,\n",
    "                      batch_size=1):\n",
    "\n",
    "  def _input_fn():\n",
    "    dataset = tf.data.TFRecordDataset(filenames=file_names)\n",
    "\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    if is_training:\n",
    "      buffer_size = batch_size * 2 + 1\n",
    "      dataset = dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "    dataset = dataset.map(parse_record)\n",
    "    dataset = dataset.map(lambda image, label: (preprocess_image(image, is_training), label))\n",
    "\n",
    "    dataset = dataset.prefetch(2 * batch_size)\n",
    "\n",
    "    # We call repeat after shuffling, rather than before, to prevent separate\n",
    "    # epochs from blending together.\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "    # Batch results by up to batch_size, and then fetch the tuple from the\n",
    "    # iterator.\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    images, labels = iterator.get_next()\n",
    "\n",
    "    features = {'images': images}\n",
    "    return features, labels\n",
    "  \n",
    "  return _input_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_columns():\n",
    "  feature_columns = {\n",
    "    'images': tf.feature_column.numeric_column('images', (IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH)),\n",
    "  }\n",
    "  return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = get_feature_columns()\n",
    "print(\"Feature Columns: {}\".format(feature_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Instantiate an Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _activation_summary(x):\n",
    "  \"\"\"Helper to create summaries for activations.\n",
    "  Creates a summary that provides a histogram of activations.\n",
    "  Creates a summary that measures the sparsity of activations.\n",
    "  Args:\n",
    "    x: Tensor\n",
    "  Returns:\n",
    "    nothing\n",
    "  \"\"\"\n",
    "  # Remove 'tower_[0-9]/' from the name in case this is a multi-GPU training\n",
    "  # session. This helps the clarity of presentation on tensorboard.\n",
    "  tensor_name = re.sub('%s_[0-9]*/' % TOWER_NAME, '', x.op.name)\n",
    "  tf.summary.histogram(tensor_name + '/activations', x)\n",
    "  tf.summary.scalar(tensor_name + '/sparsity', tf.nn.zero_fraction(x))\n",
    "  \n",
    "def _variable_on_cpu(name, shape, initializer):\n",
    "  \"\"\"Helper to create a Variable stored on CPU memory.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    initializer: initializer for Variable\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  with tf.device('/cpu:0'):\n",
    "    dtype = tf.float32\n",
    "    var = tf.get_variable(name, shape, initializer=initializer, dtype=dtype)\n",
    "  return var\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "  \"\"\"Helper to create an initialized Variable with weight decay.\n",
    "  Note that the Variable is initialized with a truncated normal distribution.\n",
    "  A weight decay is added only if one is specified.\n",
    "  Args:\n",
    "    name: name of the variable\n",
    "    shape: list of ints\n",
    "    stddev: standard deviation of a truncated Gaussian\n",
    "    wd: add L2Loss weight decay multiplied by this float. If None, weight\n",
    "        decay is not added for this Variable.\n",
    "  Returns:\n",
    "    Variable Tensor\n",
    "  \"\"\"\n",
    "  dtype = tf.float32\n",
    "  var = _variable_on_cpu(\n",
    "      name,\n",
    "      shape,\n",
    "      tf.truncated_normal_initializer(stddev=stddev, dtype=dtype))\n",
    "  if wd is not None:\n",
    "    weight_decay = tf.multiply(tf.nn.l2_loss(var), wd, name='weight_loss')\n",
    "    tf.add_to_collection('losses', weight_decay)\n",
    "  return var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images):\n",
    "  with tf.variable_scope('conv1') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 3, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(images, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.0))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv1 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv1)\n",
    "    \n",
    "  pool1 = tf.nn.max_pool(conv1, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool1')\n",
    "  norm1 = tf.nn.lrn(pool1, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm1')\n",
    "  \n",
    "  with tf.variable_scope('conv2') as scope:\n",
    "    kernel = _variable_with_weight_decay('weights', shape=[5, 5, 64, 64], stddev=5e-2, wd=0.0)\n",
    "    conv = tf.nn.conv2d(norm1, kernel, [1, 1, 1, 1], padding='SAME')\n",
    "    biases = _variable_on_cpu('biases', [64], tf.constant_initializer(0.1))\n",
    "    pre_activation = tf.nn.bias_add(conv, biases)\n",
    "    conv2 = tf.nn.relu(pre_activation, name=scope.name)\n",
    "    _activation_summary(conv2)\n",
    "\n",
    "  norm2 = tf.nn.lrn(conv2, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name='norm2')\n",
    "  pool2 = tf.nn.max_pool(norm2, ksize=[1, 3, 3, 1], strides=[1, 2, 2, 1], padding='SAME', name='pool2')\n",
    "    \n",
    "  with tf.variable_scope('local3') as scope:\n",
    "    pool2_shape = pool2.get_shape()\n",
    "    dim = pool2_shape[1] * pool2_shape[2] * pool2_shape[3]\n",
    "    reshape = tf.reshape(pool2, [-1, dim])\n",
    "    weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [384], tf.constant_initializer(0.1))\n",
    "    local3 = tf.nn.relu(tf.matmul(reshape, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local3)\n",
    "\n",
    "  with tf.variable_scope('local4') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=0.04, wd=0.004)\n",
    "    biases = _variable_on_cpu('biases', [192], tf.constant_initializer(0.1))\n",
    "    local4 = tf.nn.relu(tf.matmul(local3, weights) + biases, name=scope.name)\n",
    "    _activation_summary(local4)\n",
    "\n",
    "  with tf.variable_scope('softmax_linear') as scope:\n",
    "    weights = _variable_with_weight_decay('weights', [192, NUM_CLASSES], stddev=1/192.0, wd=0.0)\n",
    "    biases = _variable_on_cpu('biases', [NUM_CLASSES], tf.constant_initializer(0.0))\n",
    "    logits = tf.add(tf.matmul(local4, weights), biases, name=scope.name)\n",
    "    _activation_summary(logits)\n",
    "\n",
    "  return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(logits, labels):\n",
    "  # Calculate loss, which includes softmax cross entropy and L2 regularization.\n",
    "  cross_entropy = tf.losses.softmax_cross_entropy(\n",
    "    logits=logits, onehot_labels=labels)\n",
    "\n",
    "  # Create a tensor named cross_entropy for logging purposes.\n",
    "  tf.identity(cross_entropy, name='cross_entropy')\n",
    "  tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "\n",
    "  # Add weight decay to the loss.\n",
    "  loss = cross_entropy + WEIGHT_DECAY * tf.add_n(\n",
    "      [tf.nn.l2_loss(v) for v in tf.trainable_variables()])\n",
    "  \n",
    "  return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train_op(loss, params, mode):\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    # Scale the learning rate linearly with the batch size. When the batch size\n",
    "    # is 128, the learning rate should be 0.1.\n",
    "    initial_learning_rate = 0.1 * params.batch_size / 128\n",
    "    batches_per_epoch = NUM_EXAMPLES_PER_EPOCH_FOR_TRAIN / params.batch_size\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Multiply the learning rate by 0.1 at 100, 150, and 200 epochs.\n",
    "    boundaries = [int(batches_per_epoch * epoch) for epoch in [100, 150, 200]]\n",
    "    values = [initial_learning_rate * decay for decay in [1, 0.1, 0.01, 0.001]]\n",
    "    learning_rate = tf.train.piecewise_constant(\n",
    "        tf.cast(global_step, tf.int32), boundaries, values)\n",
    "\n",
    "    # Create a tensor named learning_rate for logging purposes\n",
    "    tf.identity(learning_rate, name='learning_rate')\n",
    "    tf.summary.scalar('learning_rate', learning_rate)\n",
    "\n",
    "    optimizer = tf.train.MomentumOptimizer(\n",
    "        learning_rate=learning_rate,\n",
    "        momentum=MOMENTUM)\n",
    "\n",
    "    # Batch norm requires update ops to be added as a dependency to the train_op\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "      train_op = optimizer.minimize(loss, global_step)\n",
    "  else:\n",
    "    train_op = None\n",
    "    \n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_metrics(predictions, labels):\n",
    "  # Calculate accuracy\n",
    "  accuracy = tf.metrics.accuracy(predictions['classes'],\n",
    "                                 tf.argmax(labels, axis=1))\n",
    "\n",
    "  # Create a tensor named train_accuracy for logging purposes\n",
    "  tf.identity(accuracy[1], name='train_accuracy')\n",
    "  tf.summary.scalar('train_accuracy', accuracy[1])\n",
    "  \n",
    "  return {'accuracy': accuracy}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode, params):\n",
    "  # Create the input layers from the features\n",
    "  feature_columns = list(get_feature_columns().values())\n",
    "  \n",
    "  images = tf.feature_column.input_layer(\n",
    "    features=features, feature_columns=feature_columns)\n",
    "  \n",
    "  images = tf.reshape(\n",
    "    images, shape=(-1, IMAGE_HEIGHT, IMAGE_WIDTH, IMAGE_DEPTH))\n",
    "\n",
    "  # Calculate logits through CNN\n",
    "  logits = inference(images)\n",
    "\n",
    "  # Get predictions\n",
    "  predictions = {\n",
    "    'classes': tf.argmax(logits, axis=1),\n",
    "    'probabilities': tf.nn.softmax(logits, name='softmax_tensor')\n",
    "  }\n",
    "\n",
    "  # Provide an estimator spec for `ModeKeys.PREDICT`\n",
    "  if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    export_outputs = {\n",
    "      'predictions': tf.estimator.export.PredictOutput(predictions)\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                      predictions=predictions,\n",
    "                                      export_outputs=export_outputs)\n",
    "\n",
    "  loss = get_loss(logits=logits, labels=labels)\n",
    "  train_op = get_train_op(loss=loss, mode=mode, params=params)\n",
    "  metrics = get_metrics(predictions=predictions, labels=labels)\n",
    "  \n",
    "  # Return EstimatorSpec\n",
    "  return tf.estimator.EstimatorSpec(\n",
    "    mode=mode,\n",
    "    predictions=predictions,\n",
    "    loss=loss,\n",
    "    train_op=train_op,\n",
    "    eval_metric_ops=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_estimator(run_config, hparams):\n",
    "  return tf.estimator.Estimator(\n",
    "      model_fn=model_fn,\n",
    "      params=hparams,\n",
    "      config=run_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Define Experiment Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_experiment_fn(**experiment_args):\n",
    "  def _experiment_fn(run_config, hparams):\n",
    "    return tf.contrib.learn.Experiment(\n",
    "      estimator=create_estimator(run_config, hparams),\n",
    "      train_input_fn=generate_input_fn(file_names=train_data_files,\n",
    "                                       mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "                                       num_epochs=hparams.num_epochs,\n",
    "                                       batch_size=hparams.batch_size),\n",
    "      eval_input_fn=generate_input_fn(file_names=valid_data_files,\n",
    "                                      mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                      num_epochs=hparams.num_epochs,\n",
    "                                      batch_size=hparams.batch_size),\n",
    "      **experiment_args\n",
    "    )\n",
    "  \n",
    "  return _experiment_fn  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Set HParam and RunConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 1\n",
    "BATCH_SIZE = 200\n",
    "TRAIN_SIZE = 50000\n",
    "NUM_EVAL = 1\n",
    "CHECKPOINT_STEPS = int((TRAIN_SIZE/BATCH_SIZE) * (NUM_EPOCHS/NUM_EVAL))\n",
    "\n",
    "hparams = tf.contrib.training.HParams(\n",
    "  num_epochs=NUM_EPOCHS,\n",
    "  batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "model_dir = 'trained_models/{}'.format(MODEL_NAME)\n",
    "\n",
    "run_config = tf.contrib.learn.RunConfig(\n",
    "  save_checkpoints_steps=CHECKPOINT_STEPS,\n",
    "  tf_random_seed=19851211,\n",
    "  model_dir=model_dir\n",
    ")\n",
    "\n",
    "\n",
    "print(hparams)\n",
    "print(\"Model Directory:\", run_config.model_dir)\n",
    "print(\"\")\n",
    "print(\"Dataset Size:\", TRAIN_SIZE)\n",
    "print(\"Batch Size:\", BATCH_SIZE)\n",
    "print(\"Steps per Epoch:\",TRAIN_SIZE/BATCH_SIZE)\n",
    "print(\"Total Steps:\", (TRAIN_SIZE/BATCH_SIZE)*NUM_EPOCHS)\n",
    "print(\"Required Evaluation Steps:\", NUM_EVAL) \n",
    "print(\"That is 1 evaluation step after each\",NUM_EPOCHS/NUM_EVAL,\" epochs\")\n",
    "print(\"Save Checkpoint After\",CHECKPOINT_STEPS,\"steps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Define Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def serving_input_fn():\n",
    "\n",
    "  receiver_tensor = {'images': tf.placeholder(shape=[None, 32, 32, 3], dtype=tf.float32)}\n",
    "  features = {'images': tf.map_fn(preprocess_image, receiver_tensor['images'])}\n",
    "  \n",
    "  return tf.estimator.export.ServingInputReceiver(features, receiver_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Run Experiment via learn_runner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_CHECKPOINT:\n",
    "  print(\"Removing previous artifacts...\")\n",
    "  shutil.rmtree(model_dir, ignore_errors=True)\n",
    "else:\n",
    "  print(\"Resuming training...\")\n",
    "  \n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow()\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\")\n",
    "\n",
    "learn_runner.run(\n",
    "  experiment_fn=generate_experiment_fn(\n",
    "    export_strategies=[make_export_strategy(\n",
    "        serving_input_fn,\n",
    "        exports_to_keep=1\n",
    "      )]\n",
    "  ),\n",
    "  schedule='train_and_evaluate',\n",
    "  run_config=run_config,\n",
    "  hparams=hparams\n",
    ")\n",
    "\n",
    "time_end = datetime.utcnow()\n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 1000\n",
    "valid_size = 1000\n",
    "test_size = 1000\n",
    "\n",
    "train_input_fn = generate_input_fn(file_names=train_data_files,\n",
    "                                   mode=tf.contrib.learn.ModeKeys.TRAIN,\n",
    "                                   num_epochs=None,\n",
    "                                   batch_size=train_size)\n",
    "\n",
    "valid_input_fn = generate_input_fn(file_names=valid_data_files,\n",
    "                                   mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                   num_epochs=None,\n",
    "                                   batch_size=valid_size)\n",
    "\n",
    "test_input_fn = generate_input_fn(file_names=test_data_files,\n",
    "                                  mode=tf.contrib.learn.ModeKeys.EVAL,\n",
    "                                  num_epochs=None,\n",
    "                                  batch_size=test_size)\n",
    "\n",
    "\n",
    "estimator = create_estimator(run_config, hparams)\n",
    "\n",
    "train_results = estimator.evaluate(input_fn=train_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(train_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "valid_results = estimator.evaluate(input_fn=valid_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(valid_results))\n",
    "print(\"######################################################################################\")\n",
    "\n",
    "test_results = estimator.evaluate(input_fn=test_input_fn, steps=1)\n",
    "print()\n",
    "print(\"######################################################################################\")\n",
    "print(\"# {}\".format(test_results))\n",
    "print(\"######################################################################################\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "export_dir = model_dir + '/export/Servo/'\n",
    "saved_model_dir = os.path.join(export_dir, os.listdir(export_dir)[-1]) \n",
    "\n",
    "print(saved_model_dir)\n",
    "print('')\n",
    "\n",
    "predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "  export_dir = saved_model_dir,\n",
    "  signature_def_key='predictions')\n",
    "\n",
    "N = 1000\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "with tf.gfile.Open('cifar-10/cifar-10-batches-py/test_batch', 'r') as f:\n",
    "  eval_data = cPickle.load(f)\n",
    "  \n",
    "for i in range(N):\n",
    "  x = np.random.randint(10000)\n",
    "  image = eval_data['data'][x]\n",
    "  image = np.reshape(image, [IMAGE_DEPTH, IMAGE_HEIGHT, IMAGE_WIDTH])\n",
    "  image = np.transpose(image, [1, 2, 0])\n",
    "  images.append(image)\n",
    "  labels.append(eval_data['labels'][x])\n",
    "\n",
    "output = predictor_fn(\n",
    "  {\n",
    "    'images': images,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum([a==r for a, r in zip(labels, output['classes'])]) / float(N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for pid in TensorBoard.list()['pid']:\n",
    "    TensorBoard().stop(pid)\n",
    "    print 'Stopped TensorBoard with pid {}'.format(pid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Using tf.keras for the Inference Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(images, mode):\n",
    "  if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "    tf.keras.backend.set_learning_phase(True)\n",
    "  else:\n",
    "    tf.keras.backend.set_learning_phase(False)\n",
    "        \n",
    "  model = tf.keras.models.Sequential()\n",
    "  # Define input tensor in Keras world.\n",
    "  model.add(tf.keras.layers.InputLayer(input_tensor=images))\n",
    "    \n",
    "  # The first convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "\n",
    "  # The second convolutional layer.\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=32, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.MaxPool2D(pool_size=(2, 2), padding='same'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  # The third convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "    \n",
    "  # The fourth convolutional layer\n",
    "  model.add(tf.keras.layers.Conv2D(\n",
    "      filters=64, kernel_size=(3, 3), padding='same', activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.25))\n",
    "    \n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "  model.add(tf.keras.layers.Dense(512, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dropout(0.5))\n",
    "\n",
    "  model.add(tf.keras.layers.Dense(NUM_CLASSES))\n",
    "    \n",
    "  logits = model.output\n",
    "    \n",
    "  return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
